import argparse
import base64
import importlib
import os
import pathlib
import sys

from ascend.sdk import definitions, field, value
from ascend.sdk.applier import DataServiceApplier
from ascend.sdk.client import Client

{% for mod in proto_mods %}
import ascend.protos.{{mod}}.{{mod}}_pb2 as {{mod}}
{% endfor %}

{% for gmod, cls in gmod_classes %}
from {{gmod}} import {{cls}}
{% endfor %}

# Ensure we can load python files in our sub-directories
DIR = os.path.dirname(os.path.abspath(__file__))
sys.path.append(DIR)


GENERATED_FROM_HOSTNAME = "{{hostname}}"
GENERATED_FROM_DATA_SERVICE_ID = "{{data_service.id}}"


def construct_data_service(data_service_id: str = GENERATED_FROM_DATA_SERVICE_ID):
  # Credentials for this data service
  credentials = []
  {% for credential in data_service.credentials %}
  {% set cred_var = "credential_" + (credential.id | replace("-", "_")) %}

  {{cred_var}} = {{ renderer(credential, classname_map=classname_map) | indent(2) }}
  credentials.append({{cred_var}})
  {% endfor %}


  # Connections for this data service
  connections = []
  {% for connection in data_service.connections %}
  {% set conn_var = "connection_" + (connection.id | replace("-", "_")) %}

  {{conn_var}} = {{ renderer(connection, classname_map=classname_map) | indent(2) }}
  connections.append({{conn_var}})
  {% endfor %}


  # Dynamically load dataflows for this data service by looking for sub directories with dataflows.
  # If you prefer to load specific Dataflows, you can remove this section and add specific imports like
  #
  # from Foo import dataflow as dataflow_Foo
  # dataflows.append(dataflow_Foo.construct_dataflow(data_service_id))
  dataflows = []
  for d in os.listdir(DIR):
    file = os.path.join(DIR, d, f"{d}.py")
    if os.path.exists(file):
      spec = importlib.util.spec_from_file_location("local", file)
      module_d = importlib.util.module_from_spec(spec)
      spec.loader.exec_module(module_d)
      dataflows.append(module_d.construct_dataflow(data_service_id))
  {% for dataflow in data_service.dataflows %}
  {% do write_dataflow(hostname, data_service.id, dataflow, os.path.join(base_path, dataflow.id), template_dir) %}
  {% endfor %}

  # Data service
  {% set attribute_overrides = {
        ("credentials",): "credentials",
        ("connections",): "connections",
        ("dataflows",): "dataflows",
        ("id",): "data_service_id",
        ("name",): ('"' + data_service.name.replace('"', '\\"') + '" if data_service_id == GENERATED_FROM_DATA_SERVICE_ID else data_service_id'),
      }
  %}
  return {{ renderer(data_service, classname_map=classname_map, attribute_overrides=attribute_overrides) | indent(2) }}


if __name__ == "__main__":
  parser = argparse.ArgumentParser()
  parser.add_argument("--hostname", default=os.getenv("ASCEND_HOSTNAME", default=GENERATED_FROM_HOSTNAME))
  parser.add_argument("-ds", "--data-service-id", default=os.getenv("ASCEND_DATA_SERVICE_ID", default=GENERATED_FROM_DATA_SERVICE_ID))
  parser.add_argument("--update-credentials", action='store_true')
  parser.add_argument("--update-connections", action='store_true')
  parser.add_argument("--no-delete", action='store_true')
  parser.add_argument("--dry-run", action='store_true')
  args = parser.parse_args()

  client = Client(args.hostname)
  data_service = construct_data_service(args.data_service_id)
  if not args.update_credentials:
    data_service.credentials = None
  if not args.update_connections:
    data_service.connections = None
  DataServiceApplier(client).apply(data_service, delete=(not args.no_delete), dry_run=args.dry_run)
