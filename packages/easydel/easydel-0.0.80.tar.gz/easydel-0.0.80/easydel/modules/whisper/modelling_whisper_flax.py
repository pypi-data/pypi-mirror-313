# Copyright 2023 The EASYDEL Author @erfanzar (Erfan Zare Chavoshi).
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import math
import random
from functools import partial
from typing import Any, Optional, Tuple, Union

import flax.linen
import jax
import jax.numpy as jnp
import transformers
from flax import linen as nn
from flax.core.frozen_dict import FrozenDict, freeze, unfreeze
from flax.linen import Dense, combine_masks, make_causal_mask
from flax.linen import partitioning as nn_partitioning
from flax.traverse_util import flatten_dict, unflatten_dict
from jax import lax
from jax.random import PRNGKey

from easydel.etils.etils import EasyDeLGradientCheckPointers
from easydel.inference.logits_process import (
	FlaxLogitsProcessorList,
	FlaxStaticForceTokensLogitsProcessor,
	FlaxWhisperTimeStampLogitsProcessor,
)
from easydel.layers.attention import FlaxAttentionModule, FlexibleAttentionModule

# easydel.modules
from easydel.modules.factory import register_module
from easydel.modules.flax_modeling_utils import (
	ACT2FN,
	get_dot_general_by_bits,
	get_gradient_checkpoint_policy,
)
from easydel.modules.modeling_flax_outputs import (
	FlaxBaseModelOutput,
	FlaxBaseModelOutputWithPastAndCrossAttentions,
	FlaxCausalLMOutputWithCrossAttentions,
	FlaxSeq2SeqLMOutput,
	FlaxSeq2SeqModelOutput,
	FlaxSequenceClassifierOutput,
)
from easydel.modules.modeling_utils import EasyDeLBaseModule
from easydel.modules.whisper.whisper_configuration import WhisperConfig as WhisperConfig

remat = nn_partitioning.remat


def sinusoidal_embedding_init(key, shape, dtype=jnp.float_) -> jax.Array:
	length, channels = shape
	if channels % 2 != 0:
		raise ValueError(
			f"Number of channels has to be divisible by 2 for sinusoidal positional embeddings, got {channels} channels."
		)
	log_timescale_increment = math.log(10000) / (channels // 2 - 1)
	inv_timescales = jnp.exp(-log_timescale_increment * jnp.arange(channels // 2))
	scaled_time = jnp.arange(length).reshape(-1, 1) * inv_timescales.reshape(1, -1)
	return jnp.concatenate([jnp.sin(scaled_time), jnp.cos(scaled_time)], axis=1).astype(
		dtype
	)


class FlaxWhisperAttention(FlaxAttentionModule):
	config: WhisperConfig
	embed_dim: int
	num_heads: int
	dropout: float = 0.0
	causal: bool = False
	bias: bool = True
	dtype: jnp.dtype = jnp.float32
	param_dtype: jnp.dtype = jnp.float32
	precision: Optional[Union[str, lax.Precision]] = None

	def setup(self) -> None:
		self.head_dim = self.embed_dim // self.num_heads
		if self.head_dim * self.num_heads != self.embed_dim:
			raise ValueError(
				f"embed_dim must be divisible by num_heads (got `embed_dim`: {self.embed_dim}"
				f" and `num_heads`: {self.num_heads})."
			)

		dense = partial(
			Dense,
			self.embed_dim,
			dtype=self.dtype,
			param_dtype=self.param_dtype,
			precision=self.precision,
			kernel_init=jax.nn.initializers.normal(self.config.init_std),
			**get_dot_general_by_bits(self.config.bits, self.config.easy_method),
		)

		self.q_proj = dense(use_bias=self.bias)
		self.k_proj = dense(use_bias=False)
		self.v_proj = dense(use_bias=self.bias)
		self.out_proj = dense(use_bias=self.bias)

		self.attention_performer = FlexibleAttentionModule(
			use_sharding_constraint=self.config.use_sharding_constraint,
			num_q_heads=self.num_heads,
			num_kv_heads=self.num_heads,
			attention_dropout=self.config.attention_dropout,
			head_dims=self.head_dim,
			shard_attention_computation=self.config.shard_attention_computation,
			precision=self.precision,
			force_float32_tpu=True,
			attn_mechanism=self.config.attn_mechanism,
			dtype=self.config.attn_dtype,
			partition_axis=self.config.partition_axis,
			scan_ring_attention=self.config.scan_ring_attention,
			mesh=self.config.mesh,
			sm_scale=1 / math.sqrt(self.head_dim),
			axis_name=self.config.attention_axis_name,
			backward_pass_impl=self.config.flash_attention_backward_pass_impl,
		)

	def __call__(
		self,
		hidden_states: jnp.ndarray,
		key_value_states: Optional[jnp.ndarray] = None,
		attention_mask: Optional[jnp.ndarray] = None,
		causal_mask: Optional[jnp.ndarray] = None,
		init_cache: bool = False,
		deterministic: bool = True,
	) -> tuple[Any, Any]:
		is_cross_attention = key_value_states is not None
		batch_size = hidden_states.shape[0]

		query_states = self.q_proj(hidden_states)

		if is_cross_attention:
			key_states = self.k_proj(key_value_states)
			value_states = self.v_proj(key_value_states)
		else:
			key_states = self.k_proj(hidden_states)
			value_states = self.v_proj(hidden_states)

		query_states = self._split_heads(query_states)
		key_states = self._split_heads(key_states)
		value_states = self._split_heads(value_states)

		if self.causal:
			assert causal_mask is not None, "seems like you forgot to pass causal_mask"
			query_length, key_length = query_states.shape[1], key_states.shape[1]
			if self.has_variable("cache", "cached_key"):
				mask_shift = self.variables["cache"]["cache_index"]
				max_decoder_length = self.variables["cache"]["cached_key"].shape[1]
				causal_mask = lax.dynamic_slice(
					causal_mask,
					(0, 0, mask_shift, 0),
					(1, 1, query_length, max_decoder_length),
				)
			else:
				causal_mask = causal_mask[:, :, :query_length, :key_length]
			causal_mask = jnp.broadcast_to(causal_mask, (batch_size,) + causal_mask.shape[1:])

		# combine masks if needed
		if attention_mask is not None and self.causal:
			attention_mask = jnp.broadcast_to(
				jnp.expand_dims(attention_mask, axis=(-3, -2)), causal_mask.shape
			)
			attention_mask = combine_masks(attention_mask, causal_mask)
		elif self.causal:
			attention_mask = causal_mask
		elif attention_mask is not None:
			attention_mask = jnp.expand_dims(attention_mask, axis=(-3, -2))

		# During fast autoregressive decoding, we feed one position at a time,
		# and cache the keys and values step by step.

		if self.causal and (self.has_variable("cache", "cached_key") or init_cache):
			key_states, value_states, attention_mask = self._concatenate_to_cache(
				query_states,
				key_states,
				value_states,
				attention_mask,
			)

		query_length, key_length = query_states.shape[1], key_states.shape[1]
		# Convert the boolean attention mask to an attention bias.
		if attention_mask is not None:
			# attention mask in the form of attention bias
			attention_bias = lax.select(
				attention_mask > 0,
				jnp.full(attention_mask.shape, 0.0).astype(self.dtype),
				jnp.full(attention_mask.shape, jnp.finfo(self.dtype).min).astype(self.dtype),
			)
		else:
			attention_bias = None

		dropout_rng = None
		if not deterministic and self.dropout > 0.0:
			dropout_rng = self.make_rng("dropout")

		attentions = self.attention_performer.__call__(
			query_states=query_states,
			key_states=key_states,
			value_states=value_states,
			bias=attention_bias,
			attention_mask=attention_mask,
			causal=False,
			dropout_rng=dropout_rng,
			deterministic=deterministic,
			query_sequence_length=query_length,
			key_value_sequence_length=key_length,
			uses_cache=self.has_variable("cache", "cached_key") or init_cache,
			segment_ids=None,
			causal_mask=causal_mask,
		)

		attn_output = self.shard_attention_prod(
			self._merge_heads(attentions.attention_outputs)
		)
		attn_output = self.out_proj(attn_output)

		return attn_output, attentions.attention_outputs

	def _split_heads(self, hidden_state) -> jnp.ndarray:
		return hidden_state.reshape(
			hidden_state.shape[:2] + (self.num_heads, self.head_dim)
		)

	def _merge_heads(self, hidden_state) -> jnp.ndarray:
		return hidden_state.reshape(hidden_state.shape[:2] + (self.embed_dim,))


class FlaxWhisperEncoderLayer(nn.Module):
	config: WhisperConfig
	dtype: jnp.dtype = jnp.float32
	param_dtype: jnp.dtype = jnp.float32
	precision: Optional[Union[str, lax.Precision]] = None

	def setup(self) -> None:
		self.embed_dim = self.config.d_model

		self.self_attn = FlaxWhisperAttention(
			config=self.config,
			embed_dim=self.embed_dim,
			num_heads=self.config.encoder_attention_heads,
			dropout=self.config.attention_dropout,
			dtype=self.dtype,
			param_dtype=self.param_dtype,
			precision=self.precision,
		)
		dense_class = partial(
			Dense,
			dtype=self.dtype,
			param_dtype=self.param_dtype,
			precision=self.precision,
			kernel_init=jax.nn.initializers.normal(self.config.init_std),
			**get_dot_general_by_bits(self.config.bits, self.config.easy_method),
		)
		self.self_attn_layer_norm = nn.LayerNorm(
			param_dtype=self.param_dtype,
			dtype=self.dtype,
			epsilon=1e-05,
		)
		self.dropout_layer = flax.linen.Dropout(rate=self.config.dropout)
		self.activation_fn = ACT2FN[self.config.activation_function]
		self.activation_dropout_layer = flax.linen.Dropout(
			rate=self.config.activation_dropout
		)
		self.fc1 = dense_class(self.config.encoder_ffn_dim)
		self.fc2 = dense_class(self.embed_dim)
		self.final_layer_norm = nn.LayerNorm(
			param_dtype=self.param_dtype,
			dtype=self.dtype,
			epsilon=1e-05,
		)

	def __call__(
		self,
		hidden_states: jnp.ndarray,
		attention_mask: jnp.ndarray,
		causal_mask: Optional[jnp.ndarray] = None,
		output_attentions: bool = True,
		deterministic: bool = True,
	) -> Tuple[jnp.ndarray]:
		residual = hidden_states
		hidden_states = self.self_attn_layer_norm(hidden_states)
		hidden_states, attn_weights = self.self_attn(
			hidden_states=hidden_states,
			attention_mask=attention_mask,
			causal_mask=causal_mask,
		)
		hidden_states = self.dropout_layer(hidden_states, deterministic=deterministic)
		hidden_states = residual + hidden_states

		residual = hidden_states
		hidden_states = self.final_layer_norm(hidden_states)
		hidden_states = self.activation_fn(self.fc1(hidden_states))
		hidden_states = self.activation_dropout_layer(
			hidden_states, deterministic=deterministic
		)
		hidden_states = self.fc2(hidden_states)
		hidden_states = self.dropout_layer(hidden_states, deterministic=deterministic)
		hidden_states = residual + hidden_states

		outputs = (hidden_states,)

		if output_attentions:
			outputs += (attn_weights,)

		return outputs


class FlaxWhisperEncoderLayerCollection(nn.Module):
	config: WhisperConfig
	dtype: jnp.dtype = jnp.float32
	param_dtype: jnp.dtype = jnp.float32
	precision: Optional[Union[str, lax.Precision]] = None

	def setup(self):
		block = FlaxWhisperEncoderLayer
		if self.config.gradient_checkpointing != EasyDeLGradientCheckPointers.NONE:
			block = remat(
				block,
				static_argnums=(2, 3, 4),
				policy=get_gradient_checkpoint_policy(self.config.gradient_checkpointing),
			)
		self.layers = [
			block(
				self.config,
				name=str(i),
				dtype=self.dtype,
				param_dtype=self.param_dtype,
				precision=self.precision,
			)
			for i in range(self.config.encoder_layers)
		]
		self.layerdrop = self.config.encoder_layerdrop

	def __call__(
		self,
		hidden_states,
		attention_mask,
		causal_mask: Optional[jnp.ndarray] = None,
		deterministic: bool = True,
		output_attentions: bool = False,
		output_hidden_states: bool = False,
		return_dict: bool = True,
	):
		all_attentions = () if output_attentions else None
		all_hidden_states = () if output_hidden_states else None

		for encoder_layer in self.layers:
			if output_hidden_states:
				all_hidden_states = all_hidden_states + (hidden_states,)
			dropout_probability = random.uniform(0, 1)
			if not deterministic and (dropout_probability < self.layerdrop):  # skip the layer
				layer_outputs = (None, None)
			else:
				layer_outputs = encoder_layer(
					hidden_states,
					attention_mask,
					causal_mask,
					output_attentions,
					deterministic,
				)
			hidden_states = layer_outputs[0]
			if output_attentions:
				all_attentions = all_attentions + (layer_outputs[1],)

		if output_hidden_states:
			all_hidden_states += (hidden_states,)

		outputs = (hidden_states, all_hidden_states, all_attentions)

		if not return_dict:
			return tuple(v for v in outputs if v is not None)

		return FlaxBaseModelOutput(
			last_hidden_state=hidden_states,
			hidden_states=all_hidden_states,
			attentions=all_attentions,
		)


class FlaxWhisperDecoderLayer(nn.Module):
	config: WhisperConfig
	dtype: jnp.dtype = jnp.float32
	param_dtype: jnp.dtype = jnp.float32
	precision: Optional[Union[str, lax.Precision]] = None

	def setup(self) -> None:
		self.embed_dim = self.config.d_model
		self.self_attn = FlaxWhisperAttention(
			config=self.config,
			embed_dim=self.embed_dim,
			num_heads=self.config.decoder_attention_heads,
			dropout=self.config.attention_dropout,
			causal=True,
			dtype=self.dtype,
			param_dtype=self.param_dtype,
			precision=self.precision,
		)
		self.dropout_layer = flax.linen.Dropout(rate=self.config.dropout)
		self.activation_fn = ACT2FN[self.config.activation_function]
		self.activation_dropout_layer = flax.linen.Dropout(
			rate=self.config.activation_dropout
		)

		self.self_attn_layer_norm = nn.LayerNorm(
			param_dtype=self.param_dtype,
			dtype=self.dtype,
			epsilon=1e-05,
		)
		self.encoder_attn = FlaxWhisperAttention(
			config=self.config,
			embed_dim=self.embed_dim,
			num_heads=self.config.decoder_attention_heads,
			dropout=self.config.attention_dropout,
			dtype=self.dtype,
			param_dtype=self.param_dtype,
			precision=self.precision,
		)
		self.encoder_attn_layer_norm = nn.LayerNorm(
			param_dtype=self.param_dtype,
			dtype=self.dtype,
			epsilon=1e-05,
		)
		dense_class = partial(
			Dense,
			param_dtype=self.param_dtype,
			precision=self.precision,
			dtype=self.dtype,
			kernel_init=jax.nn.initializers.normal(self.config.init_std),
			**get_dot_general_by_bits(self.config.bits, self.config.easy_method),
		)
		self.fc1 = dense_class(self.config.decoder_ffn_dim)
		self.fc2 = dense_class(self.embed_dim)
		self.final_layer_norm = nn.LayerNorm(
			param_dtype=self.param_dtype,
			dtype=self.dtype,
			epsilon=1e-05,
		)

	def __call__(
		self,
		hidden_states: jnp.ndarray,
		attention_mask: jnp.ndarray,
		causal_mask: Optional[jnp.ndarray] = None,
		encoder_hidden_states: Optional[jnp.ndarray] = None,
		encoder_attention_mask: Optional[jnp.ndarray] = None,
		init_cache: bool = False,
		output_attentions: bool = True,
		deterministic: bool = True,
	) -> Tuple[jnp.ndarray]:
		residual = hidden_states
		hidden_states = self.self_attn_layer_norm(hidden_states)

		# Self Attention
		hidden_states, self_attn_weights = self.self_attn(
			hidden_states=hidden_states,
			attention_mask=attention_mask,
			causal_mask=causal_mask,
			init_cache=init_cache,
		)
		hidden_states = self.dropout_layer(hidden_states, deterministic=deterministic)
		hidden_states = residual + hidden_states

		# Cross-Attention Block
		cross_attn_weights = None
		if encoder_hidden_states is not None:
			residual = hidden_states

			hidden_states = self.encoder_attn_layer_norm(hidden_states)
			hidden_states, cross_attn_weights = self.encoder_attn(
				hidden_states=hidden_states,
				causal_mask=causal_mask,
				key_value_states=encoder_hidden_states,
				attention_mask=encoder_attention_mask,
			)
			hidden_states = self.dropout_layer(hidden_states, deterministic=deterministic)
			hidden_states = residual + hidden_states

		# Fully Connected
		residual = hidden_states
		hidden_states = self.final_layer_norm(hidden_states)
		hidden_states = self.activation_fn(self.fc1(hidden_states))
		hidden_states = self.activation_dropout_layer(
			hidden_states, deterministic=deterministic
		)
		hidden_states = self.fc2(hidden_states)
		hidden_states = self.dropout_layer(hidden_states, deterministic=deterministic)
		hidden_states = residual + hidden_states

		outputs = (hidden_states,)

		if output_attentions:
			outputs += (self_attn_weights, cross_attn_weights)

		return outputs


class FlaxWhisperDecoderLayerCollection(nn.Module):
	config: WhisperConfig
	dtype: jnp.dtype = jnp.float32
	param_dtype: jnp.dtype = jnp.float32
	precision: Optional[Union[str, lax.Precision]] = None

	def setup(self):
		block = FlaxWhisperDecoderLayer
		if self.config.gradient_checkpointing != EasyDeLGradientCheckPointers.NONE:
			block = remat(
				block,
				static_argnums=(2, 5, 6, 7),
				policy=get_gradient_checkpoint_policy(self.config.gradient_checkpointing),
			)
		self.layers = [
			block(
				self.config,
				name=str(i),
				dtype=self.dtype,
				param_dtype=self.param_dtype,
				precision=self.precision,
			)
			for i in range(self.config.decoder_layers)
		]

		self.layerdrop = self.config.decoder_layerdrop

	def __call__(
		self,
		hidden_states,
		attention_mask,
		causal_mask: Optional[jnp.ndarray] = None,
		encoder_hidden_states: Optional[jnp.ndarray] = None,
		encoder_attention_mask: Optional[jnp.ndarray] = None,
		deterministic: bool = True,
		init_cache: bool = False,
		output_attentions: bool = False,
		output_hidden_states: bool = False,
		return_dict: bool = True,
	):
		# decoder layers
		all_hidden_states = () if output_hidden_states else None
		all_self_attns = () if output_attentions else None
		all_cross_attentions = (
			() if (output_attentions and encoder_hidden_states is not None) else None
		)

		for decoder_layer in self.layers:
			if output_hidden_states:
				all_hidden_states += (hidden_states,)
				# add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)
			dropout_probability = random.uniform(0, 1)
			if not deterministic and (dropout_probability < self.layerdrop):
				layer_outputs = (None, None, None)
			else:
				layer_outputs = decoder_layer(
					hidden_states,
					attention_mask,
					causal_mask,
					encoder_hidden_states,
					encoder_attention_mask,
					init_cache,
					output_attentions,
					deterministic,
				)

			hidden_states = layer_outputs[0]
			if output_attentions:
				all_self_attns += (layer_outputs[1],)

				if encoder_hidden_states is not None:
					all_cross_attentions += (layer_outputs[2],)

		# add hidden states from the last decoder layer
		if output_hidden_states:
			all_hidden_states += (hidden_states,)

		outputs = [
			hidden_states,
			all_hidden_states,
			all_self_attns,
			all_cross_attentions,
		]

		if not return_dict:
			return tuple(v for v in outputs if v is not None)

		return FlaxBaseModelOutputWithPastAndCrossAttentions(
			last_hidden_state=hidden_states,
			hidden_states=all_hidden_states,
			attentions=all_self_attns,
			cross_attentions=all_cross_attentions,
		)


class FlaxWhisperEncoder(nn.Module):
	config: WhisperConfig
	dtype: jnp.dtype = jnp.float32
	param_dtype: jnp.dtype = jnp.float32
	precision: Optional[Union[str, lax.Precision]] = None

	def setup(self) -> None:
		self.conv1 = nn.Conv(
			self.config.d_model,
			kernel_size=(3,),
			padding=1,
			kernel_init=jax.nn.initializers.normal(self.config.init_std),
			dtype=self.dtype,
			param_dtype=self.param_dtype,
			precision=self.precision,
		)
		self.conv2 = nn.Conv(
			self.config.d_model,
			kernel_size=(3,),
			strides=2,
			padding=1,
			kernel_init=jax.nn.initializers.normal(self.config.init_std),
			dtype=self.dtype,
			param_dtype=self.param_dtype,
			precision=self.precision,
		)

		self.dropout_layer = flax.linen.Dropout(rate=self.config.dropout)

		self.layers = FlaxWhisperEncoderLayerCollection(
			self.config,
			dtype=self.dtype,
			param_dtype=self.param_dtype,
			precision=self.precision,
		)

		self.embed_positions = nn.Embed(
			self.config.max_source_positions,
			self.config.d_model,
			dtype=self.dtype,
			embedding_init=sinusoidal_embedding_init,
			param_dtype=self.param_dtype,
		)

		self.layer_norm = nn.LayerNorm(
			param_dtype=self.param_dtype,
			dtype=self.dtype,
			epsilon=1e-05,
		)
		self.causal_mask = make_causal_mask(
			jnp.ones(
				(
					1,
					max(
						self.config.max_source_positions,
						self.config.max_target_positions,
					),
				),
				dtype="bool",
			),
			dtype="bool",
		)

	def __call__(
		self,
		input_features: jnp.ndarray,
		output_attentions: bool = False,
		output_hidden_states: bool = False,
		return_dict: bool = True,
		deterministic: bool = True,
	) -> tuple[Any | None, ...] | FlaxBaseModelOutput:
		if input_features.shape[1:] != (
			self.config.num_mel_bins,
			self.config.max_source_positions * 2,
		):
			raise ValueError(
				"input_features.shape[1:], must be equal to (self.config.num_mel_bins,"
				f" self.config.max_source_positions * 2) (got {input_features.shape[1:]}, but should be"
				f" ({self.config.num_mel_bins}, {self.config.max_source_positions * 2}))"
			)

		input_features = input_features.transpose(0, 2, 1)
		hidden_states = jax.nn.gelu(self.conv1(input_features), approximate=False)
		hidden_states = jax.nn.gelu(self.conv2(hidden_states), approximate=False)

		embed_positions = self.embed_positions(jnp.arange(self.config.max_source_positions))
		# freeze the sinusoidal embeddings by stopping the back-prop
		embed_positions = jax.lax.stop_gradient(embed_positions)
		hidden_states = hidden_states + embed_positions

		hidden_states = self.dropout_layer(hidden_states, deterministic=deterministic)

		outputs = self.layers(
			hidden_states,
			causal_mask=self.causal_mask,
			attention_mask=None,
			deterministic=deterministic,
			output_attentions=output_attentions,
			output_hidden_states=output_hidden_states,
			return_dict=return_dict,
		)

		last_hidden_states = outputs[0]
		last_hidden_states = self.layer_norm(last_hidden_states)

		# update the last element in `hidden_states` after applying `layernorm` above
		hidden_states = None
		if output_hidden_states:
			hidden_states = outputs[1]
			hidden_states = hidden_states[:-1] + (last_hidden_states,)

		if not return_dict:
			outputs = (last_hidden_states, hidden_states) + (
				outputs[2:] if output_hidden_states else outputs[1:]
			)
			return tuple(v for v in outputs if v is not None)

		return FlaxBaseModelOutput(
			last_hidden_state=last_hidden_states,
			hidden_states=hidden_states,
			attentions=outputs.attentions,
		)


class FlaxWhisperDecoder(nn.Module):
	config: WhisperConfig
	dtype: jnp.dtype = jnp.float32
	param_dtype: jnp.dtype = jnp.float32
	precision: Optional[Union[str, lax.Precision]] = None

	def setup(self) -> None:
		self.embed_tokens = nn.Embed(
			self.config.vocab_size,
			self.config.d_model,
			dtype=self.dtype,
			param_dtype=self.param_dtype,
		)
		self.embed_positions = nn.Embed(
			self.config.max_target_positions,
			self.config.d_model,
			dtype=self.dtype,
			param_dtype=self.param_dtype,
		)

		self.layers = FlaxWhisperDecoderLayerCollection(
			self.config,
			dtype=self.dtype,
			param_dtype=self.param_dtype,
			precision=self.precision,
		)

		self.dropout_layer = flax.linen.Dropout(rate=self.config.dropout)

		self.layer_norm = nn.LayerNorm(
			param_dtype=self.param_dtype,
			dtype=self.dtype,
			epsilon=1e-05,
		)
		self.causal_mask = make_causal_mask(
			jnp.ones(
				(
					1,
					max(
						self.config.max_source_positions,
						self.config.max_target_positions,
					),
				),
				dtype="bool",
			),
			dtype="bool",
		)

	def __call__(
		self,
		input_ids: jnp.ndarray,
		attention_mask: jnp.ndarray,
		position_ids: jnp.ndarray,
		encoder_hidden_states: Optional[jnp.ndarray] = None,
		init_cache: bool = False,
		output_attentions: bool = False,
		output_hidden_states: bool = False,
		return_dict: bool = True,
		deterministic: bool = True,
	) -> tuple[Any, ...] | FlaxBaseModelOutputWithPastAndCrossAttentions:
		input_embeds = self.embed_tokens(input_ids)
		position_embeds = self.embed_positions(position_ids)

		hidden_states = input_embeds + position_embeds
		hidden_states = self.dropout_layer(hidden_states, deterministic=deterministic)

		outputs = self.layers(
			hidden_states,
			attention_mask=attention_mask,
			causal_mask=self.causal_mask,
			encoder_hidden_states=encoder_hidden_states,
			deterministic=deterministic,
			init_cache=init_cache,
			output_attentions=output_attentions,
			output_hidden_states=output_hidden_states,
			return_dict=return_dict,
		)

		last_hidden_states = outputs[0]
		last_hidden_states = self.layer_norm(last_hidden_states)

		hidden_states = None
		if output_hidden_states:
			hidden_states = outputs[1]
			hidden_states = hidden_states[:-1] + (last_hidden_states,)

		if not return_dict:
			outputs = (last_hidden_states, hidden_states) + (
				outputs[2:] if output_hidden_states else outputs[1:]
			)
			return tuple(v for v in outputs if v is not None)

		return FlaxBaseModelOutputWithPastAndCrossAttentions(
			last_hidden_state=last_hidden_states,
			hidden_states=hidden_states,
			attentions=outputs.attentions,
			cross_attentions=outputs.cross_attentions,
		)


class FlaxWhisperModule(nn.Module):
	config: WhisperConfig
	dtype: jnp.dtype = jnp.float32
	param_dtype: jnp.dtype = jnp.float32
	precision: Optional[Union[str, lax.Precision]] = None

	def setup(self) -> None:
		self.encoder = FlaxWhisperEncoder(
			self.config,
			dtype=self.dtype,
			param_dtype=self.param_dtype,
			precision=self.precision,
		)
		self.decoder = FlaxWhisperDecoder(
			self.config,
			dtype=self.dtype,
			param_dtype=self.param_dtype,
			precision=self.precision,
		)

	def _get_decoder_module(self):
		return self.decoder

	def _get_encoder_module(self):
		return self.encoder

	def __call__(
		self,
		input_features: jnp.ndarray,
		decoder_input_ids: jnp.ndarray,
		decoder_attention_mask: jnp.ndarray,
		decoder_position_ids: jnp.ndarray,
		output_attentions: bool = False,
		output_hidden_states: bool = False,
		return_dict: bool = True,
		deterministic: bool = True,
	):
		encoder_outputs = self.encoder(
			input_features,
			output_attentions=output_attentions,
			output_hidden_states=output_hidden_states,
			return_dict=return_dict,
			deterministic=deterministic,
		)

		decoder_outputs = self.decoder(
			input_ids=decoder_input_ids,
			attention_mask=decoder_attention_mask,
			position_ids=decoder_position_ids,
			encoder_hidden_states=encoder_outputs[0],
			output_attentions=output_attentions,
			output_hidden_states=output_hidden_states,
			return_dict=return_dict,
			deterministic=deterministic,
		)

		if not return_dict:
			return decoder_outputs + encoder_outputs

		return FlaxSeq2SeqModelOutput(
			last_hidden_state=decoder_outputs.last_hidden_state,
			decoder_hidden_states=decoder_outputs.hidden_states,
			decoder_attentions=decoder_outputs.attentions,
			cross_attentions=decoder_outputs.cross_attentions,
			encoder_last_hidden_state=encoder_outputs.last_hidden_state,
			encoder_hidden_states=encoder_outputs.hidden_states,
			encoder_attentions=encoder_outputs.attentions,
		)

	def _get_encoder_module(self):
		return self.encoder

	def _get_decoder_module(self):
		return self.decoder


class FlaxWhisperPreTrainedModel(EasyDeLBaseModule):
	config_class = WhisperConfig
	base_model_prefix: str = "model"
	main_input_name = "input_features"
	module_class: nn.Module = None

	def __init__(
		self,
		config: WhisperConfig,
		dtype: jnp.dtype = jnp.float32,
		param_dtype: jnp.dtype = jnp.float32,
		precision: Optional[jax.lax.Precision] = None,
		input_shape: Optional[Tuple[int, int, int]] = None,
		seed: int = 0,
		_do_init: bool = False,
		**kwargs,
	):
		"""
		Initializes the pre-trained model with the given configuration.

		Args:
		    config (LlamaConfig): Configuration for the model.
		    dtype (jnp.dtype): Data type for computations.
		    param_dtype (jnp.dtype): Data type for model parameters.
		    precision (Optional[jax.lax.Precision]): Precision setting for JAX operations.
		    input_shape (Tuple[int, int]): Shape of the input tensor.
		    seed (int): Seed for random number generation.
		    _do_init (bool): If True, initialize model weights.
		    **kwargs: Additional keyword arguments.
		"""
		if input_shape is None:
			input_shape = (
				1,
				config.num_mel_bins,
				config.max_source_positions * 2,
			)
		super().__init__(
			config=config,
			dtype=dtype,
			param_dtype=param_dtype,
			precision=precision,
			input_shape=input_shape,
			seed=seed,
			_do_init=_do_init,
			**kwargs,
		)

	def init_weights(
		self,
		rng: jax.random.PRNGKey,
		input_shape: Tuple,
		params: FrozenDict = None,
	) -> FrozenDict:
		# init input tensors
		input_features = jnp.zeros(input_shape, dtype="f4")
		input_features = input_features.at[(..., -1)].set(self.config.eos_token_id)

		decoder_input_ids = jnp.zeros((input_shape[0], 1), dtype="i4")
		decoder_attention_mask = jnp.ones_like(decoder_input_ids)

		batch_size, sequence_length = decoder_input_ids.shape
		decoder_position_ids = jnp.broadcast_to(
			jnp.arange(sequence_length)[None, :], (batch_size, sequence_length)
		)

		params_rng, dropout_rng = jax.random.split(rng)
		rngs = {"params": params_rng, "dropout": dropout_rng}

		random_params = self.module.init(
			rngs,
			input_features=input_features,
			decoder_input_ids=decoder_input_ids,
			decoder_attention_mask=decoder_attention_mask,
			decoder_position_ids=decoder_position_ids,
		)["params"]

		if params is not None:
			random_params = flatten_dict(unfreeze(random_params))
			params = flatten_dict(unfreeze(params))
			for missing_key in self._missing_keys:
				params[missing_key] = random_params[missing_key]
			self._missing_keys = set()
			return freeze(unflatten_dict(params))
		else:
			return random_params

	def init_cache(self, batch_size, max_length, encoder_outputs):
		decoder_input_ids = jnp.ones((batch_size, max_length), dtype="i4")
		decoder_attention_mask = jnp.ones_like(decoder_input_ids)
		decoder_position_ids = jnp.broadcast_to(
			jnp.arange(jnp.atleast_2d(decoder_input_ids).shape[-1]),
			decoder_input_ids.shape,
		)

		def _decoder_forward(
			module,
			decoder_input_ids,
			decoder_attention_mask,
			decoder_position_ids,
			**kwargs,
		):
			decoder_module = module._get_decoder_module()
			return decoder_module(
				decoder_input_ids,
				decoder_attention_mask,
				decoder_position_ids,
				**kwargs,
			)

		init_variables = self.module.init(
			jax.random.PRNGKey(0),
			decoder_input_ids=decoder_input_ids,
			decoder_attention_mask=decoder_attention_mask,
			decoder_position_ids=decoder_position_ids,
			encoder_hidden_states=encoder_outputs[0],
			init_cache=True,
			method=_decoder_forward,
		)
		return unfreeze(init_variables["cache"])

	def encode(
		self,
		input_features: jnp.ndarray,
		attention_mask: Optional[jnp.ndarray] = None,
		output_attentions: Optional[bool] = None,
		output_hidden_states: Optional[bool] = None,
		return_dict: Optional[bool] = None,
		train: bool = False,
		params: dict = None,
		dropout_rng: PRNGKey = None,
		add_params_field: bool = False,
		**kwargs,
	):
		output_attentions = (
			output_attentions
			if output_attentions is not None
			else self.config.output_attentions
		)
		output_hidden_states = (
			output_hidden_states
			if output_hidden_states is not None
			else self.config.output_hidden_states
		)
		return_dict = return_dict if return_dict is not None else self.config.return_dict

		rngs = {}
		if dropout_rng is not None:
			rngs["dropout"] = dropout_rng

		def _encoder_forward(module, input_features, **kwargs):
			encode_module = module._get_encoder_module()
			return encode_module(input_features, **kwargs)

		return self.module.apply(
			(
				{"params": params or self.params} if add_params_field else params or self.params
			),
			input_features=jnp.array(input_features, dtype="f4"),
			output_attentions=output_attentions,
			output_hidden_states=output_hidden_states,
			return_dict=return_dict,
			deterministic=not train,
			rngs=rngs,
			method=_encoder_forward,
		)

	def decode(
		self,
		decoder_input_ids,
		encoder_outputs,
		encoder_attention_mask: Optional[jnp.ndarray] = None,
		decoder_attention_mask: Optional[jnp.ndarray] = None,
		decoder_position_ids: Optional[jnp.ndarray] = None,
		past_key_values: Optional[dict] = None,
		output_attentions: Optional[bool] = None,
		output_hidden_states: Optional[bool] = None,
		return_dict: Optional[bool] = None,
		train: bool = False,
		params: dict = None,
		dropout_rng: PRNGKey = None,
		add_params_field: bool = False,
	):
		output_attentions = (
			output_attentions
			if output_attentions is not None
			else self.config.output_attentions
		)
		output_hidden_states = (
			output_hidden_states
			if output_hidden_states is not None
			else self.config.output_hidden_states
		)
		return_dict = return_dict if return_dict is not None else self.config.return_dict

		encoder_hidden_states = encoder_outputs[0]

		batch_size, sequence_length = decoder_input_ids.shape
		if decoder_position_ids is None:
			if past_key_values is not None:
				raise ValueError(
					"Make sure to provide `decoder_position_ids` when passing `past_key_values`."
				)

			if decoder_attention_mask is not None:
				decoder_position_ids = (
					decoder_attention_mask.cumsum(-1) * decoder_attention_mask
				) - 1
			else:
				decoder_position_ids = jnp.broadcast_to(
					jnp.arange(sequence_length)[None, :], (batch_size, sequence_length)
				)

		if decoder_attention_mask is None:
			decoder_attention_mask = jnp.ones((batch_size, sequence_length))

		rngs = {}
		if dropout_rng is not None:
			rngs["dropout"] = dropout_rng

		inputs = (
			{"params": params or self.params} if add_params_field else params or self.params
		)

		if past_key_values is not None:
			inputs["cache"] = past_key_values
			mutable = ["cache"]
		else:
			mutable = False

		def _decoder_forward(
			module,
			decoder_input_ids,
			decoder_attention_mask,
			decoder_position_ids,
			**kwargs,
		):
			decoder_module = module._get_decoder_module()
			return decoder_module(
				input_ids=decoder_input_ids,
				attention_mask=decoder_attention_mask,
				position_ids=decoder_position_ids,
				**kwargs,
			)

		outputs = self.module.apply(
			inputs,
			decoder_input_ids=jnp.array(decoder_input_ids, dtype="i4"),
			decoder_attention_mask=jnp.array(decoder_attention_mask, dtype="i4"),
			decoder_position_ids=jnp.array(decoder_position_ids, dtype="i4"),
			encoder_hidden_states=encoder_hidden_states,
			output_attentions=output_attentions,
			output_hidden_states=output_hidden_states,
			return_dict=return_dict,
			deterministic=not train,
			rngs=rngs,
			mutable=mutable,
			method=_decoder_forward,
		)

		# add updated cache to model output
		if past_key_values is not None and return_dict:
			outputs, past = outputs
			outputs["past_key_values"] = unfreeze(past["cache"])
			return outputs
		elif past_key_values is not None and not return_dict:
			outputs, past = outputs
			outputs = outputs[:1] + (unfreeze(past["cache"]),) + outputs[1:]

		return outputs

	def __call__(
		self,
		input_features: jnp.ndarray,
		decoder_input_ids: jnp.ndarray,
		decoder_attention_mask: Optional[jnp.ndarray] = None,
		decoder_position_ids: Optional[jnp.ndarray] = None,
		output_attentions: Optional[bool] = None,
		output_hidden_states: Optional[bool] = None,
		return_dict: Optional[bool] = None,
		train: bool = False,
		params: dict = None,
		dropout_rng: PRNGKey = None,
		add_params_field: bool = False,
		**kwargs,
	):
		output_attentions = (
			output_attentions
			if output_attentions is not None
			else self.config.output_attentions
		)
		output_hidden_states = (
			output_hidden_states
			if output_hidden_states is not None
			else self.config.output_hidden_states
		)
		return_dict = return_dict if return_dict is not None else self.config.return_dict

		# prepare decoder inputs
		if decoder_position_ids is None:
			if decoder_attention_mask is not None:
				decoder_position_ids = (
					decoder_attention_mask.cumsum(-1) * decoder_attention_mask
				) - 1
			else:
				batch_size, sequence_length = decoder_input_ids.shape
				decoder_position_ids = jnp.broadcast_to(
					jnp.arange(sequence_length)[None, :], (batch_size, sequence_length)
				)
		if decoder_attention_mask is None:
			decoder_attention_mask = jnp.ones_like(decoder_input_ids)

		# Handle any PRNG if needed
		rngs = {"dropout": dropout_rng} if dropout_rng is not None else {}

		return self.module.apply(
			(
				{"params": params or self.params} if add_params_field else params or self.params
			),
			input_features=jnp.array(input_features, dtype="f4"),
			decoder_input_ids=jnp.array(decoder_input_ids, dtype="i4"),
			decoder_attention_mask=jnp.array(decoder_attention_mask, dtype="i4"),
			decoder_position_ids=jnp.array(decoder_position_ids, dtype="i4"),
			output_attentions=output_attentions,
			output_hidden_states=output_hidden_states,
			return_dict=return_dict,
			deterministic=not train,
			rngs=rngs,
		)


@register_module(
	"base-module",
	config=WhisperConfig,
	model_type="whisper",
	embedding_layer_names=["embed_positions", "embed_tokens"],
	layernorm_names=[
		"self_attn_layer_norm",
		"final_layer_norm",
		"encoder_attn_layer_norm",
		"layer_norm",
	],
)
class FlaxWhisperModel(FlaxWhisperPreTrainedModel):
	config: WhisperConfig
	module_class = FlaxWhisperModule


class FlaxWhisperForConditionalGenerationModule(nn.Module):
	config: WhisperConfig
	dtype: jnp.dtype = jnp.float32
	param_dtype: jnp.dtype = jnp.float32
	precision: Optional[Union[str, lax.Precision]] = None

	def setup(self) -> None:
		self.model = FlaxWhisperModule(
			config=self.config,
			dtype=self.dtype,
			param_dtype=self.param_dtype,
			precision=self.precision,
		)
		self.proj_out = Dense(
			self.config.vocab_size,
			use_bias=False,
			dtype=self.dtype,
			param_dtype=self.param_dtype,
			precision=self.precision,
			kernel_init=jax.nn.initializers.normal(self.config.init_std),
			**get_dot_general_by_bits(self.config.bits, self.config.easy_method),
		)

	def _get_encoder_module(self):
		return self.model.encoder

	def _get_decoder_module(self):
		return self.model.decoder

	def __call__(
		self,
		input_features,
		decoder_input_ids,
		decoder_attention_mask: jnp.ndarray = None,
		decoder_position_ids: jnp.ndarray = None,
		output_attentions: bool = False,
		output_hidden_states: bool = False,
		return_dict: bool = True,
		deterministic: bool = True,
	):
		outputs = self.model(
			input_features=input_features,
			decoder_input_ids=decoder_input_ids,
			decoder_attention_mask=decoder_attention_mask,
			decoder_position_ids=decoder_position_ids,
			output_attentions=output_attentions,
			output_hidden_states=output_hidden_states,
			return_dict=return_dict,
			deterministic=deterministic,
		)

		hidden_states = outputs[0]

		if self.config.tie_word_embeddings:
			shared_embedding = self.model.decoder.embed_tokens.variables["params"][
				"embedding"
			].T.astype(self.param_dtype)
			lm_logits = self.proj_out.apply(
				{"params": {"kernel": shared_embedding}},
				hidden_states,
			)
		else:
			lm_logits = self.proj_out(hidden_states)

		if not return_dict:
			output = (lm_logits,) + outputs[1:]
			return output

		return FlaxSeq2SeqLMOutput(
			logits=lm_logits,
			decoder_hidden_states=outputs.decoder_hidden_states,
			decoder_attentions=outputs.decoder_attentions,
			cross_attentions=outputs.cross_attentions,
			encoder_last_hidden_state=outputs.encoder_last_hidden_state,
			encoder_hidden_states=outputs.encoder_hidden_states,
			encoder_attentions=outputs.encoder_attentions,
		)


@register_module(
	"seq-to-seq",
	config=WhisperConfig,
	model_type="whisper",
	embedding_layer_names=["embed_positions", "embed_tokens"],
	layernorm_names=[
		"self_attn_layer_norm",
		"final_layer_norm",
		"encoder_attn_layer_norm",
		"layer_norm",
	],
)
class FlaxWhisperForConditionalGeneration(FlaxWhisperPreTrainedModel):
	module_class = FlaxWhisperForConditionalGenerationModule

	def decode(
		self,
		decoder_input_ids,
		encoder_outputs,
		encoder_attention_mask: Optional[jnp.ndarray] = None,
		decoder_attention_mask: Optional[jnp.ndarray] = None,
		decoder_position_ids: Optional[jnp.ndarray] = None,
		past_key_values: Optional[dict] = None,
		output_attentions: Optional[bool] = None,
		output_hidden_states: Optional[bool] = None,
		return_dict: Optional[bool] = None,
		train: bool = False,
		params: dict = None,
		dropout_rng: PRNGKey = None,
		add_params_field: Optional[bool] = False,
	):
		output_attentions = (
			output_attentions
			if output_attentions is not None
			else self.config.output_attentions
		)
		output_hidden_states = (
			output_hidden_states
			if output_hidden_states is not None
			else self.config.output_hidden_states
		)
		return_dict = return_dict if return_dict is not None else self.config.return_dict

		encoder_hidden_states = encoder_outputs[0]

		batch_size, sequence_length = decoder_input_ids.shape
		if decoder_position_ids is None:
			if past_key_values is not None:
				raise ValueError(
					"Make sure to provide `decoder_position_ids` when passing `past_key_values`."
				)

			if decoder_attention_mask is not None:
				decoder_position_ids = (
					decoder_attention_mask.cumsum(-1) * decoder_attention_mask
				) - 1
			else:
				decoder_position_ids = jnp.broadcast_to(
					jnp.arange(sequence_length)[None, :], (batch_size, sequence_length)
				)
		if decoder_attention_mask is None:
			decoder_attention_mask = jnp.ones((batch_size, sequence_length), dtype="i4")

		# Handle any PRNG if needed
		rngs = {}
		if dropout_rng is not None:
			rngs["dropout"] = dropout_rng

		inputs = (
			{"params": params or self.params} if add_params_field else params or self.params
		)

		if past_key_values is not None:
			inputs["cache"] = past_key_values
			mutable = ["cache"]
		else:
			mutable = False

		def _decoder_forward(
			module,
			decoder_input_ids,
			decoder_attention_mask,
			decoder_position_ids,
			**kwargs,
		):
			decoder_module = module._get_decoder_module()
			outputs = decoder_module(
				input_ids=decoder_input_ids,
				attention_mask=decoder_attention_mask,
				position_ids=decoder_position_ids,
				**kwargs,
			)
			hidden_states = outputs[0]

			if self.config.tie_word_embeddings:
				shared_embedding = module.model.decoder.embed_tokens.variables["params"][
					"embedding"
				].T.astype(self.param_dtype)
				lm_logits = module.proj_out.apply(
					{"params": {"kernel": shared_embedding}},
					hidden_states,
				)
			else:
				lm_logits = module.proj_out(hidden_states)

			return lm_logits, outputs

		outputs = self.module.apply(
			inputs,
			decoder_input_ids=jnp.array(decoder_input_ids, dtype="i4"),
			decoder_attention_mask=jnp.array(decoder_attention_mask, dtype="i4"),
			decoder_position_ids=jnp.array(decoder_position_ids, dtype="i4"),
			encoder_hidden_states=encoder_hidden_states,
			output_attentions=output_attentions,
			output_hidden_states=output_hidden_states,
			return_dict=return_dict,
			deterministic=not train,
			rngs=rngs,
			mutable=mutable,
			method=_decoder_forward,
		)

		if past_key_values is None:
			lm_logits, decoder_outputs = outputs
		else:
			(lm_logits, decoder_outputs), past = outputs

		if return_dict:
			outputs = FlaxCausalLMOutputWithCrossAttentions(
				logits=lm_logits,
				hidden_states=decoder_outputs.hidden_states,
				attentions=decoder_outputs.attentions,
				cross_attentions=decoder_outputs.cross_attentions,
			)
		else:
			outputs = (lm_logits,) + decoder_outputs[1:]

		# add updated cache to model output
		if past_key_values is not None and return_dict:
			outputs["past_key_values"] = unfreeze(past["cache"])
			return outputs
		elif past_key_values is not None and not return_dict:
			outputs = outputs[:1] + (unfreeze(past["cache"]),) + outputs[1:]

		return outputs

	def generate(
		self,
		input_features,
		generation_config=None,
		logits_processor=None,
		return_timestamps=None,
		task=None,
		language=None,
		is_multilingual=None,
		**kwargs,
	):
		if generation_config is None:
			generation_config = self.generation_config

		if return_timestamps is not None:
			generation_config.return_timestamps = return_timestamps

		if task is not None:
			generation_config.task = task

		if is_multilingual is not None:
			generation_config.is_multilingual = is_multilingual

		if language is not None:
			generation_config.language = language

		if kwargs is not None and "decoder_input_ids" in kwargs:
			decoder_input_length = len(kwargs["decoder_input_ids"])
		else:
			decoder_input_length = 1

		forced_decoder_ids = []

		if (
			hasattr(generation_config, "is_multilingual")
			and generation_config.is_multilingual
		):
			if hasattr(generation_config, "language"):
				forced_decoder_ids.append(
					(1, generation_config.lang_to_id[generation_config.language])
				)
			else:
				forced_decoder_ids.append((1, None))

			if hasattr(generation_config, "task"):
				forced_decoder_ids.append(
					(2, generation_config.task_to_id[generation_config.task])
				)
			else:
				forced_decoder_ids.append((2, generation_config.task_to_id["transcribe"]))

		if (
			hasattr(generation_config, "return_timestamps")
			and generation_config.return_timestamps
		) or return_timestamps:
			logits_processor = [
				FlaxWhisperTimeStampLogitsProcessor(
					generation_config, self.config, decoder_input_length
				)
			]
		else:
			if (
				forced_decoder_ids
				and forced_decoder_ids[-1][0] != generation_config.no_timestamps_token_id
			):
				idx = forced_decoder_ids[-1][0] + 1 if forced_decoder_ids else 1
				forced_decoder_ids.append((idx, generation_config.no_timestamps_token_id))

		if len(forced_decoder_ids) > 0:
			generation_config.forced_decoder_ids = forced_decoder_ids

		return super().generate(
			input_features,
			generation_config,
			logits_processor=logits_processor,
			**kwargs,
		)

	def _force_generate(
		self,
		input_features: jax.Array,
		forced_decoder_ids: jax.Array,
		params: dict,
		return_timestamps: bool = False,
		generation_config: Optional[transformers.GenerationConfig] = None,
		**kwargs,
	):
		# fmt:off
		if generation_config is None:
			generation_config = self.generation_config
		generation_config.forced_decoder_ids = None
		logits_processor = FlaxLogitsProcessorList()
		logits_processor.append(FlaxStaticForceTokensLogitsProcessor(forced_decoder_ids))
		if return_timestamps:
			logits_processor.append(FlaxWhisperTimeStampLogitsProcessor(generation_config, self.config, 1))
		return super().generate(
			input_features,
			generation_config,
			logits_processor=logits_processor,
			params=params,
			**kwargs,
		)
		# fmt:on

	def prepare_inputs_for_generation(
		self,
		decoder_input_ids,
		max_length,
		attention_mask: Optional[jax.Array] = None,
		decoder_attention_mask: Optional[jax.Array] = None,
		encoder_outputs=None,
		**kwargs,
	):
		# initializing the cache
		batch_size, seq_length = decoder_input_ids.shape

		past_key_values = self.init_cache(batch_size, max_length, encoder_outputs)
		extended_attention_mask = jnp.ones((batch_size, max_length), dtype="i4")
		if decoder_attention_mask is not None:
			position_ids = decoder_attention_mask.cumsum(-1) - 1
			extended_attention_mask = lax.dynamic_update_slice(
				extended_attention_mask, decoder_attention_mask, (0, 0)
			)
		else:
			position_ids = jnp.broadcast_to(
				jnp.arange(seq_length, dtype="i4")[None, :], (batch_size, seq_length)
			)

		return {
			"past_key_values": past_key_values,
			"encoder_outputs": encoder_outputs,
			"encoder_attention_mask": attention_mask,
			"decoder_attention_mask": extended_attention_mask,
			"decoder_position_ids": position_ids,
		}

	def update_inputs_for_generation(self, model_outputs, model_kwargs):
		model_kwargs["past_key_values"] = model_outputs.past_key_values
		model_kwargs["decoder_position_ids"] = (
			model_kwargs["decoder_position_ids"][:, -1:] + 1
		)
		return model_kwargs


class FlaxWhisperForAudioClassificationModule(nn.Module):
	config: WhisperConfig
	dtype: jnp.dtype = jnp.float32
	param_dtype: jnp.dtype = jnp.float32
	precision: Optional[Union[str, lax.Precision]] = None

	def setup(self) -> None:
		self.encoder = FlaxWhisperEncoder(
			config=self.config,
			dtype=self.dtype,
			param_dtype=self.param_dtype,
			precision=self.precision,
		)
		self.config.is_encoder_decoder = False
		num_layers = self.config.num_hidden_layers + 1
		if self.config.use_weighted_layer_sum:
			self.layer_weights = jnp.repeat(1 / num_layers, num_layers)
		self.projector = Dense(
			self.config.classifier_proj_size,
			dtype=self.dtype,
			param_dtype=self.param_dtype,
			precision=self.precision,
			**get_dot_general_by_bits(self.config.bits, self.config.easy_method),
		)
		self.classifier = Dense(
			self.config.num_labels,
			dtype=self.dtype,
			param_dtype=self.param_dtype,
			precision=self.precision,
			**get_dot_general_by_bits(self.config.bits, self.config.easy_method),
		)

	def __call__(
		self,
		input_features,
		encoder_outputs=None,
		output_attentions=None,
		output_hidden_states: bool = True,
		return_dict: bool = True,
	):
		output_attentions = (
			output_attentions
			if output_attentions is not None
			else self.config.output_attentions
		)
		output_hidden_states = (
			output_hidden_states
			if output_hidden_states is not None
			else self.config.output_hidden_states
		)
		return_dict = (
			return_dict if return_dict is not None else self.config.use_return_dict
		)

		if encoder_outputs is None:
			encoder_outputs = self.encoder(
				input_features,
				output_attentions=output_attentions,
				output_hidden_states=output_hidden_states,
				return_dict=return_dict,
			)

		if self.config.use_weighted_layer_sum:
			hidden_states = jnp.stack(encoder_outputs, axis=1)
			norm_weights = jax.nn.softmax(self.layer_weights, axis=-1)
			hidden_states = jnp.sum(
				hidden_states * jnp.reshape(norm_weights, [-1, 1, 1]), axis=1
			)
		else:
			hidden_states = encoder_outputs[0]

		hidden_states = self.projector(hidden_states)
		pooled_output = jnp.mean(hidden_states, axis=1)

		logits = self.classifier(pooled_output)

		if not return_dict:
			return (logits,) + encoder_outputs[1:]

		return FlaxSequenceClassifierOutput(
			logits=logits,
			hidden_states=encoder_outputs.hidden_states,
			attentions=encoder_outputs.attentions,
		)


@register_module(
	"audio-classification",
	config=WhisperConfig,
	model_type="whisper",
	embedding_layer_names=["embed_positions", "embed_tokens"],
	layernorm_names=[
		"self_attn_layer_norm",
		"final_layer_norm",
		"encoder_attn_layer_norm",
		"layer_norm",
	],
)
class FlaxWhisperForAudioClassification(FlaxWhisperPreTrainedModel):
	module_class = FlaxWhisperForAudioClassificationModule

	def init_weights(
		self,
		rng: jax.random.PRNGKey,
		input_shape: Tuple,
		params: FrozenDict = None,
	) -> FrozenDict:
		# init input tensors
		input_features = jnp.zeros(input_shape, dtype="f4")
		input_features = input_features.at[(..., -1)].set(self.config.eos_token_id)
		params_rng, dropout_rng = jax.random.split(rng)
		rngs = {"params": params_rng, "dropout": dropout_rng}
		random_params = self.module.init(rngs, input_features=input_features)["params"]
		if params is not None:
			random_params = flatten_dict(unfreeze(random_params))
			params = flatten_dict(unfreeze(params))
			for missing_key in self._missing_keys:
				params[missing_key] = random_params[missing_key]
			self._missing_keys = set()
			return freeze(unflatten_dict(params))
		else:
			return random_params

	def __call__(
		self,
		input_features: jnp.ndarray,
		attention_mask: Optional[jnp.ndarray] = None,
		output_attentions: Optional[bool] = None,
		output_hidden_states: Optional[bool] = None,
		return_dict: Optional[bool] = None,
		train: bool = False,
		params: dict = None,
		dropout_rng: PRNGKey = None,
		add_params_field: Optional[bool] = False,
		**kwargs,
	):
		output_attentions = (
			output_attentions
			if output_attentions is not None
			else self.config.output_attentions
		)
		output_hidden_states = (
			output_hidden_states
			if output_hidden_states is not None
			else self.config.output_hidden_states
		)
		return_dict = return_dict if return_dict is not None else self.config.return_dict

		# Handle any PRNG if needed
		rngs = {}
		if dropout_rng is not None:
			rngs["dropout"] = dropout_rng

		return self.module.apply(
			(
				{"params": params or self.params} if add_params_field else params or self.params
			),
			input_features=jnp.array(input_features, dtype="f4"),
			output_attentions=output_attentions,
			output_hidden_states=output_hidden_states,
			return_dict=return_dict,
			rngs=rngs,
		)
