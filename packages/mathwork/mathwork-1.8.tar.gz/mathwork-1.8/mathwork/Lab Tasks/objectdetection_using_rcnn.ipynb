{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 1193394,
          "sourceType": "datasetVersion",
          "datasetId": 679312
        }
      ],
      "dockerImageVersionId": 30162,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<h1 style=\"background-color:#EEEEEE; color:#222831; text-align:left; font-family:Georgia\">Object Detection using RCNN</h1>"
      ],
      "metadata": {
        "id": "u9mGOQYsDP5q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training a typical object detection model would involve the following steps:\n",
        "1. Creating ground truth data that contains labels of the bounding box and class corresponding to various objects present in the image.\n",
        "2. A mechanism to scan through the image to identify regions (region proposals) that are likely to contain objects. Here, we use selectivesearch (module) for it\n",
        "3. Creating the target class variable by using the IoU metric.\n",
        "4. Creating the target bounding box offset variable to make corrections to the location of region proposal coming in the second step. The offset here is the delta difference between the ground truth bounding box and the region proposal through selectivesearch\n",
        "5. Building a model that can predict the class of object along with the target bounding box offset corresponding to the region proposal.\n",
        "6. Measuring the accuracy of object detection using mean Average Precision (mAP)."
      ],
      "metadata": {
        "id": "C2Ahvm2lDP5r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q --upgrade selectivesearch torch_snippets"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-08T07:33:18.000919Z",
          "iopub.execute_input": "2024-10-08T07:33:18.001267Z",
          "iopub.status.idle": "2024-10-08T07:35:48.478947Z",
          "shell.execute_reply.started": "2024-10-08T07:33:18.001167Z",
          "shell.execute_reply": "2024-10-08T07:35:48.477869Z"
        },
        "trusted": true,
        "id": "EAXZoTvBDP5t",
        "outputId": "4741b488-f4e9-4325-eca0-7523b43fe1b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x785d3267b690>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/selectivesearch/\u001b[0m\n\u001b[33mWARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x785d3267ba90>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/selectivesearch/\u001b[0m\n\u001b[33mWARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x785d3267bdd0>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/selectivesearch/\u001b[0m\n\u001b[33mWARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x785d32694150>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/selectivesearch/\u001b[0m\n\u001b[33mWARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x785d32694490>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/selectivesearch/\u001b[0m\n\u001b[31mERROR: Could not find a version that satisfies the requirement selectivesearch (from versions: none)\u001b[0m\n\u001b[31mERROR: No matching distribution found for selectivesearch\u001b[0m\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_snippets import *\n",
        "import selectivesearch\n",
        "from torchvision import transforms, models, datasets\n",
        "from torch_snippets import Report\n",
        "from torchvision.ops import nms\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "pIOo-A9m3_Th",
        "execution": {
          "iopub.status.busy": "2022-02-27T07:34:18.632888Z",
          "iopub.execute_input": "2022-02-27T07:34:18.633153Z",
          "iopub.status.idle": "2022-02-27T07:34:25.868937Z",
          "shell.execute_reply.started": "2022-02-27T07:34:18.633125Z",
          "shell.execute_reply": "2022-02-27T07:34:25.868162Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "IMAGE_ROOT = '../input/open-images-bus-trucks/images/images'\n",
        "DF_RAW = pd.read_csv('../input/open-images-bus-trucks/df.csv')\n",
        "print(DF_RAW.head())"
      ],
      "metadata": {
        "id": "14-PnjIp4Le_",
        "execution": {
          "iopub.status.busy": "2022-02-27T07:34:25.87032Z",
          "iopub.execute_input": "2022-02-27T07:34:25.870566Z",
          "iopub.status.idle": "2022-02-27T07:34:26.004029Z",
          "shell.execute_reply.started": "2022-02-27T07:34:25.870534Z",
          "shell.execute_reply": "2022-02-27T07:34:26.00329Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class OpenImages(Dataset):\n",
        "    def __init__(self, df, image_folder=IMAGE_ROOT):\n",
        "        self.root = image_folder\n",
        "        self.df = df\n",
        "        self.unique_images = df['ImageID'].unique()\n",
        "    def __len__(self): return len(self.unique_images)\n",
        "    def __getitem__(self, ix):\n",
        "        image_id = self.unique_images[ix]\n",
        "        image_path = f'{self.root}/{image_id}.jpg'\n",
        "        image = cv2.imread(image_path, 1)[...,::-1] # convert BGR to RGB\n",
        "        # Basically x[...], it is similar to cv2.imread(image_path, 1)[:, :, ::-1], but this kinda works with any dimensions by mentioning just three dots\n",
        "        h, w, _ = image.shape\n",
        "        df = self.df.copy()\n",
        "        df = df[df['ImageID'] == image_id]\n",
        "        boxes = df['XMin,YMin,XMax,YMax'.split(',')].values\n",
        "        boxes = (boxes * np.array([w,h,w,h])).astype(np.uint16).tolist() #boxes in accordance to the image\n",
        "        classes = df['LabelName'].values.tolist()\n",
        "        return image, boxes, classes, image_path\n",
        "ds = OpenImages(df=DF_RAW)\n"
      ],
      "metadata": {
        "id": "P-kZg83t5rwY",
        "execution": {
          "iopub.status.busy": "2022-02-27T07:34:26.005173Z",
          "iopub.execute_input": "2022-02-27T07:34:26.006381Z",
          "iopub.status.idle": "2022-02-27T07:34:26.02243Z",
          "shell.execute_reply.started": "2022-02-27T07:34:26.00634Z",
          "shell.execute_reply": "2022-02-27T07:34:26.021674Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For some images, there are multiple bounding boxes that may/may not belong to the same class. Below is an example:"
      ],
      "metadata": {
        "id": "mrhtntwjDP50"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "im, bbs, clss, _ = ds[6]\n",
        "show(im, bbs=bbs, texts=clss, sz=10)\n",
        "print(bbs)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-27T07:34:26.025124Z",
          "iopub.execute_input": "2022-02-27T07:34:26.025479Z",
          "iopub.status.idle": "2022-02-27T07:34:26.394601Z",
          "shell.execute_reply.started": "2022-02-27T07:34:26.025425Z",
          "shell.execute_reply": "2022-02-27T07:34:26.393426Z"
        },
        "trusted": true,
        "id": "wErfn1VzDP50"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "im, bbs, clss, _ = ds[15]\n",
        "show(im, bbs=bbs, texts=clss, sz=10)\n",
        "print(bbs)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-27T07:34:26.395588Z",
          "iopub.execute_input": "2022-02-27T07:34:26.39584Z",
          "iopub.status.idle": "2022-02-27T07:34:26.68801Z",
          "shell.execute_reply.started": "2022-02-27T07:34:26.395805Z",
          "shell.execute_reply": "2022-02-27T07:34:26.687378Z"
        },
        "trusted": true,
        "id": "huKJ3U07DP51"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dimensions of the bounding box of the bus object is best understood in the following way -\n",
        "\n",
        "![42193EA1-F670-417C-B5CD-37714DDBD836_1_201_a.jpeg](attachment:5c1fc7d3-4237-4da7-bbc0-212b74f78c5a.jpeg)"
      ],
      "metadata": {
        "id": "ErQNrOX1DP51"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2 style=\"background-color:#EEEEEE; color:#222831; text-align:left; font-family:Georgia\">SelectiveSearch to generate region proposals</h2>\n",
        "\n",
        "SelectiveSearch is a region proposal algorithm used for object localization where it generates proposals of regions that are likely to be grouped together based on their pixel intensities. SelectiveSearch groups pixels based on the hierarchical grouping of similar pixels, which, in turn, leverages the color, texture, size, and shape compatibility of content within an image."
      ],
      "metadata": {
        "id": "s_AfEgfUDP52"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_candidates(img):\n",
        "    img_lbl, regions = selectivesearch.selective_search(img, scale=200, min_size=100)\n",
        "    img_area = np.prod(img.shape[:2])\n",
        "    candidates = []\n",
        "    for r in regions:\n",
        "        if r['rect'] in candidates: continue\n",
        "        if r['size'] < (0.05*img_area): continue\n",
        "        if r['size'] > (1*img_area): continue\n",
        "        x, y, w, h = r['rect']\n",
        "        candidates.append(list(r['rect']))\n",
        "    return candidates\n",
        "\n",
        "#how iou works\n",
        "def extract_iou(boxA, boxB, epsilon=1e-5):\n",
        "    x1 = max(boxA[0], boxB[0])\n",
        "    y1 = max(boxA[1], boxB[1])\n",
        "    x2 = min(boxA[2], boxB[2])\n",
        "    y2 = min(boxA[3], boxB[3])\n",
        "    width = (x2 - x1)  #change in x-direction\n",
        "    height = (y2 - y1)  #change in y-direction\n",
        "    if (width<0) or (height <0):\n",
        "        return 0.0\n",
        "    area_overlap = width * height # this was calculated by x1, y1, x2, y2\n",
        "    area_a = (boxA[2] - boxA[0]) * (boxA[3] - boxA[1])\n",
        "    area_b = (boxB[2] - boxB[0]) * (boxB[3] - boxB[1])\n",
        "    area_combined = area_a + area_b - area_overlap\n",
        "    iou = area_overlap / (area_combined+epsilon)\n",
        "    return iou"
      ],
      "metadata": {
        "id": "bt_LSq5_55TZ",
        "execution": {
          "iopub.status.busy": "2022-02-27T07:34:26.689514Z",
          "iopub.execute_input": "2022-02-27T07:34:26.689959Z",
          "iopub.status.idle": "2022-02-27T07:34:26.703424Z",
          "shell.execute_reply.started": "2022-02-27T07:34:26.689924Z",
          "shell.execute_reply": "2022-02-27T07:34:26.702644Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Selectivesearch module\n",
        "\n",
        "The selectivesearch creates multiple candidate bounding boxes randomly, to be passed along with the input image. The IoU (Intersection over Union) is the loss metric here, hence, the bounding box with the highest IoU to the original bounding box target will be accepted."
      ],
      "metadata": {
        "id": "eZqCIFLfDP53"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of df[15]\n",
        "candidates = extract_candidates(im)\n",
        "print(np.shape(candidates))\n",
        "print(type(candidates))\n",
        "show(im, bbs = candidates)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-27T07:34:26.704484Z",
          "iopub.execute_input": "2022-02-27T07:34:26.704683Z",
          "iopub.status.idle": "2022-02-27T07:34:27.174145Z",
          "shell.execute_reply.started": "2022-02-27T07:34:26.704659Z",
          "shell.execute_reply": "2022-02-27T07:34:27.173162Z"
        },
        "trusted": true,
        "id": "r4nw8yEaDP54"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "help(show)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-27T07:34:27.175702Z",
          "iopub.execute_input": "2022-02-27T07:34:27.176063Z",
          "iopub.status.idle": "2022-02-27T07:34:27.181892Z",
          "shell.execute_reply.started": "2022-02-27T07:34:27.176025Z",
          "shell.execute_reply": "2022-02-27T07:34:27.180933Z"
        },
        "trusted": true,
        "id": "_dcw3F_1DP54"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.shape(ds[15])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-27T07:34:27.183298Z",
          "iopub.execute_input": "2022-02-27T07:34:27.183637Z",
          "iopub.status.idle": "2022-02-27T07:34:27.204911Z",
          "shell.execute_reply.started": "2022-02-27T07:34:27.183599Z",
          "shell.execute_reply": "2022-02-27T07:34:27.204103Z"
        },
        "trusted": true,
        "id": "m4yV7S5hDP54"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "(im, bbs, labels, fpath) = ds[15]\n",
        "H, W, _ = im.shape\n",
        "candidates = extract_candidates(im)\n",
        "candidates = np.array([(x,y,x+w,y+h) for x,y,w,h in candidates])  # candidates extracted are the x,y,w,h way and made x1, y1, x2, y2 to be similar to the bbs in the dataset\n",
        "\n",
        "ious, rois, clss, deltas, best_ious = [], [], [], [], []\n",
        "temp_best_bbs = []\n",
        "ious = np.array([[extract_iou(candidate, _bb_) for candidate in candidates] for _bb_ in bbs]).T\n",
        "\n",
        "for jx, candidate in enumerate(candidates):\n",
        "    cx,cy,cX,cY = candidate\n",
        "    candidate_ious = ious[jx]  #ious for that candidate\n",
        "    best_iou_at = np.argmax(candidate_ious)  #best candidate iou is taken (index) ~ always be a zero index\n",
        "    best_iou = candidate_ious[best_iou_at]   #gets the best score here\n",
        "    best_ious.append(best_iou)\n",
        "    best_bb = _x,_y,_X,_Y = bbs[best_iou_at] # gets the target label bounding box where there is the highest iou\n",
        "    temp_best_bbs.append(best_bb)\n",
        "    if best_iou > 0.3: clss.append(labels[best_iou_at]) # if iou is more than 0.3 it is not the background\n",
        "    else : clss.append('background')\n",
        "    delta = np.array([_x-cx, _y-cy, _X-cX, _Y-cY]) / np.array([W,H,W,H])  #normalizing the delta based on image size\n",
        "    deltas.append(delta)\n",
        "    rois.append(candidate / np.array([W,H,W,H]))\n",
        "\n",
        "best_ious_at = np.argmax(best_ious)\n",
        "print(\"Best IoU:\", best_ious[best_ious_at])\n",
        "\n",
        "\n",
        "best_candidate = candidates[best_ious_at]\n",
        "best_bbs = temp_best_bbs[best_ious_at]\n",
        "\n",
        "# Example of df[15]\n",
        "candidates = extract_candidates(im)\n",
        "show(im, bbs = [best_bbs, best_candidate], confs= [0,0.5], texts = ['Bbox', 'Best candidate Bbox'])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-27T07:34:27.206289Z",
          "iopub.execute_input": "2022-02-27T07:34:27.206609Z",
          "iopub.status.idle": "2022-02-27T07:34:28.054352Z",
          "shell.execute_reply.started": "2022-02-27T07:34:27.206574Z",
          "shell.execute_reply": "2022-02-27T07:34:28.053619Z"
        },
        "trusted": true,
        "id": "JZJkM3xDDP55"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "FPATHS, GTBBS, CLSS, DELTAS, ROIS, IOUS = [], [], [], [], [], []\n",
        "N = 500\n",
        "for ix, (im, bbs, labels, fpath) in enumerate(ds):\n",
        "    if(ix==N):\n",
        "        break\n",
        "    H, W, _ = im.shape\n",
        "    candidates = extract_candidates(im)\n",
        "    candidates = np.array([(x,y,x+w,y+h) for x,y,w,h in candidates])\n",
        "    ious, rois, clss, deltas = [], [], [], []\n",
        "    ious = np.array([[extract_iou(candidate, _bb_) for candidate in candidates] for _bb_ in bbs]).T\n",
        "    for jx, candidate in enumerate(candidates):\n",
        "        cx,cy,cX,cY = candidate\n",
        "        candidate_ious = ious[jx]\n",
        "        best_iou_at = np.argmax(candidate_ious)\n",
        "        best_iou = candidate_ious[best_iou_at]\n",
        "        best_bb = _x,_y,_X,_Y = bbs[best_iou_at]\n",
        "        # if iou is more than 0.3 it is not the background\n",
        "        if best_iou > 0.3: clss.append(labels[best_iou_at])\n",
        "        else : clss.append('background')\n",
        "        delta = np.array([_x-cx, _y-cy, _X-cX, _Y-cY]) / np.array([W,H,W,H])\n",
        "        deltas.append(delta)\n",
        "        rois.append(candidate / np.array([W,H,W,H]))\n",
        "    FPATHS.append(fpath)\n",
        "    IOUS.append(ious)\n",
        "    ROIS.append(rois)\n",
        "    CLSS.append(clss)\n",
        "    DELTAS.append(deltas)\n",
        "    GTBBS.append(bbs)\n",
        "FPATHS = [f'{IMAGE_ROOT}/{stem(f)}.jpg' for f in FPATHS]\n",
        "FPATHS, GTBBS, CLSS, DELTAS, ROIS = [item for item in [FPATHS, GTBBS, CLSS, DELTAS, ROIS]] #?"
      ],
      "metadata": {
        "id": "TtCQPF8J6CGB",
        "execution": {
          "iopub.status.busy": "2022-02-27T07:34:28.055821Z",
          "iopub.execute_input": "2022-02-27T07:34:28.056083Z",
          "iopub.status.idle": "2022-02-27T07:37:49.021209Z",
          "shell.execute_reply.started": "2022-02-27T07:34:28.056048Z",
          "shell.execute_reply": "2022-02-27T07:37:49.02047Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "targets = pd.DataFrame(flatten(CLSS), columns=['label'])\n",
        "label2target = {l:t for t,l in enumerate(targets['label'].unique())}\n",
        "target2label = {t:l for l,t in label2target.items()}\n",
        "background_class = label2target['background']\n",
        "\n",
        "\n",
        "\n",
        "print(\"The label to target values dictionary formed is:\" ,label2target)"
      ],
      "metadata": {
        "id": "yxzZs0Gs7bQt",
        "execution": {
          "iopub.status.busy": "2022-02-27T07:37:49.022658Z",
          "iopub.execute_input": "2022-02-27T07:37:49.022894Z",
          "iopub.status.idle": "2022-02-27T07:37:49.03495Z",
          "shell.execute_reply.started": "2022-02-27T07:37:49.022862Z",
          "shell.execute_reply": "2022-02-27T07:37:49.034147Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# normalizing with the mean, std used while training the model\n",
        "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                 std=[0.229, 0.224, 0.225])\n",
        "def preprocess_image(img):\n",
        "    img = torch.tensor(img).permute(2,0,1)\n",
        "    img = normalize(img)\n",
        "    return img.to(device).float()\n",
        "def decode(_y):\n",
        "    _, preds = _y.max(-1)\n",
        "    return preds\n"
      ],
      "metadata": {
        "id": "vEtHtZOO725v",
        "execution": {
          "iopub.status.busy": "2022-02-27T07:37:49.038968Z",
          "iopub.execute_input": "2022-02-27T07:37:49.039522Z",
          "iopub.status.idle": "2022-02-27T07:37:49.045096Z",
          "shell.execute_reply.started": "2022-02-27T07:37:49.039493Z",
          "shell.execute_reply": "2022-02-27T07:37:49.04429Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RCNNDataset(Dataset):\n",
        "    def __init__(self, fpaths, rois, labels, deltas, gtbbs):\n",
        "        self.fpaths = fpaths\n",
        "        self.gtbbs = gtbbs\n",
        "        self.rois = rois\n",
        "        self.labels = labels\n",
        "        self.deltas = deltas\n",
        "    def __len__(self): return len(self.fpaths)\n",
        "    def __getitem__(self, ix):\n",
        "        fpath = str(self.fpaths[ix])\n",
        "        image = cv2.imread(fpath, 1)[...,::-1]\n",
        "        H, W, _ = image.shape\n",
        "        sh = np.array([W,H,W,H])\n",
        "        gtbbs = self.gtbbs[ix]\n",
        "        rois = self.rois[ix]\n",
        "        bbs = (np.array(rois)*sh).astype(np.uint16)\n",
        "        labels = self.labels[ix]\n",
        "        deltas = self.deltas[ix]\n",
        "        crops = [image[y:Y,x:X] for (x,y,X,Y) in bbs]  # bounding box image crops\n",
        "        return image, crops, bbs, labels, deltas, gtbbs, fpath\n",
        "    def collate_fn(self, batch):\n",
        "        '''Performing actions on a batch of images'''\n",
        "        input, rois, rixs, labels, deltas = [], [], [], [], []\n",
        "        for ix in range(len(batch)):\n",
        "            image, crops, image_bbs, image_labels, image_deltas, image_gt_bbs, image_fpath = batch[ix]\n",
        "            crops = [cv2.resize(crop, (224,224)) for crop in crops]\n",
        "            crops = [preprocess_image(crop/255.)[None] for crop in crops]\n",
        "            input.extend(crops)\n",
        "            labels.extend([label2target[c] for c in image_labels])\n",
        "            deltas.extend(image_deltas)\n",
        "        input = torch.cat(input).to(device)\n",
        "        labels = torch.Tensor(labels).long().to(device)\n",
        "        deltas = torch.Tensor(deltas).float().to(device)\n",
        "        return input, labels, deltas\n"
      ],
      "metadata": {
        "id": "4vLi9hII7-WS",
        "execution": {
          "iopub.status.busy": "2022-02-27T07:37:49.046384Z",
          "iopub.execute_input": "2022-02-27T07:37:49.046798Z",
          "iopub.status.idle": "2022-02-27T07:37:49.061915Z",
          "shell.execute_reply.started": "2022-02-27T07:37:49.046759Z",
          "shell.execute_reply": "2022-02-27T07:37:49.060951Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_train = 9*len(FPATHS)//10 # 0.9 is the train size\n",
        "train_ds = RCNNDataset(FPATHS[:n_train], ROIS[:n_train], CLSS[:n_train], DELTAS[:n_train], GTBBS[:n_train])\n",
        "test_ds = RCNNDataset(FPATHS[n_train:], ROIS[n_train:], CLSS[n_train:], DELTAS[n_train:], GTBBS[n_train:])\n",
        "\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "train_loader = DataLoader(train_ds, batch_size=2, collate_fn=train_ds.collate_fn, drop_last=True)\n",
        "test_loader = DataLoader(test_ds, batch_size=2, collate_fn=test_ds.collate_fn, drop_last=True)"
      ],
      "metadata": {
        "id": "dzwT5C-J8G0j",
        "execution": {
          "iopub.status.busy": "2022-02-27T07:37:49.063298Z",
          "iopub.execute_input": "2022-02-27T07:37:49.063764Z",
          "iopub.status.idle": "2022-02-27T07:37:49.074272Z",
          "shell.execute_reply.started": "2022-02-27T07:37:49.063725Z",
          "shell.execute_reply": "2022-02-27T07:37:49.073392Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vgg_backbone = models.vgg16(pretrained=True)\n",
        "vgg_backbone.classifier = nn.Sequential()\n",
        "for param in vgg_backbone.parameters():\n",
        "    param.requires_grad = False #not to do a re-train\n",
        "vgg_backbone.eval().to(device)"
      ],
      "metadata": {
        "id": "QcLxxBJm8HUz",
        "execution": {
          "iopub.status.busy": "2022-02-27T07:37:49.077359Z",
          "iopub.execute_input": "2022-02-27T07:37:49.078311Z",
          "iopub.status.idle": "2022-02-27T07:38:11.730382Z",
          "shell.execute_reply.started": "2022-02-27T07:37:49.078268Z",
          "shell.execute_reply": "2022-02-27T07:38:11.729085Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2 style=\"background-color:#EEEEEE; color:#222831; text-align:left; font-family:Georgia\">R-CNN network architecture</h2>\n",
        "\n",
        "The following strategy is adopted for R-CNN network architecture\n",
        "\n",
        "1. Define a VGG backbone.\n",
        "2. Fetch the features post passing the normalized crop through a pretrained model.\n",
        "3. Attach a linear layer with sigmoid activation to the VGG backbone to predict the class corresponding to the region proposal.\n",
        "4. Attach an additional linear layer to predict the four bounding box offsets.\n",
        "5. Define the loss calculations for each of the two outputs (one to predict class and the other to predict the four bounding box offsets).\n",
        "6. Train the model that predicts both the class of region proposal and the four bounding box offsets."
      ],
      "metadata": {
        "id": "BoyuPgoNDP59"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        feature_dim = 25088\n",
        "        self.backbone = vgg_backbone\n",
        "        self.cls_score = nn.Linear(feature_dim, len(label2target))\n",
        "        self.bbox = nn.Sequential(\n",
        "              nn.Linear(feature_dim, 512),\n",
        "              nn.ReLU(),\n",
        "              nn.Linear(512, 4),\n",
        "              nn.Tanh(),\n",
        "            )\n",
        "        self.cel = nn.CrossEntropyLoss() # loss for classification\n",
        "        self.sl1 = nn.L1Loss() # loss for regression\n",
        "    def forward(self, input):\n",
        "        feat = self.backbone(input)  # both classification and regression takes 'feat' as input\n",
        "        cls_score = self.cls_score(feat)\n",
        "        bbox = self.bbox(feat)\n",
        "        return cls_score, bbox\n",
        "    def calc_loss(self, probs, _deltas, labels, deltas):\n",
        "        # probs is basically the predicted class\n",
        "        detection_loss = self.cel(probs, labels)\n",
        "        ixs, = torch.where(labels != 1) #removing the label 1, which is background\n",
        "        _deltas = _deltas[ixs]\n",
        "        deltas = deltas[ixs]\n",
        "        self.lmb = 10.0\n",
        "        if len(ixs) > 0:\n",
        "            regression_loss = self.sl1(_deltas, deltas)\n",
        "            return detection_loss + self.lmb * regression_loss, detection_loss.detach(), regression_loss.detach()\n",
        "        else:\n",
        "            # every ix is detected as background\n",
        "            regression_loss = 0\n",
        "            return detection_loss + self.lmb * regression_loss, detection_loss.detach(), regression_loss"
      ],
      "metadata": {
        "id": "Fs3XfT418aGk",
        "execution": {
          "iopub.status.busy": "2022-02-27T07:38:11.733159Z",
          "iopub.execute_input": "2022-02-27T07:38:11.733707Z",
          "iopub.status.idle": "2022-02-27T07:38:11.744833Z",
          "shell.execute_reply.started": "2022-02-27T07:38:11.733668Z",
          "shell.execute_reply": "2022-02-27T07:38:11.744181Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_batch(inputs, model, optimizer, criterion):\n",
        "    input, clss, deltas = inputs\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    _clss, _deltas = model(input)  # as model outputs we will be getting classes and delta (bbox offsets)\n",
        "    loss, loc_loss, regr_loss = criterion(_clss, _deltas, clss, deltas)\n",
        "    accs = clss == decode(_clss)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.detach(), loc_loss, regr_loss, accs.cpu().numpy()"
      ],
      "metadata": {
        "id": "MiMmezgp9E-s",
        "execution": {
          "iopub.status.busy": "2022-02-27T07:38:11.746423Z",
          "iopub.execute_input": "2022-02-27T07:38:11.746702Z",
          "iopub.status.idle": "2022-02-27T07:38:11.757644Z",
          "shell.execute_reply.started": "2022-02-27T07:38:11.746667Z",
          "shell.execute_reply": "2022-02-27T07:38:11.756758Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def validate_batch(inputs, model, criterion):\n",
        "    input, clss, deltas = inputs\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        _clss,_deltas = model(input)\n",
        "        loss, loc_loss, regr_loss = criterion(_clss, _deltas, clss, deltas)\n",
        "        _, _clss = _clss.max(-1)  # more like a softmax np argmax\n",
        "        accs = clss == _clss\n",
        "    return _clss, _deltas, loss.detach(), loc_loss, regr_loss, accs.cpu().numpy()"
      ],
      "metadata": {
        "id": "vNBqA98I9G6O",
        "execution": {
          "iopub.status.busy": "2022-02-27T07:38:11.758981Z",
          "iopub.execute_input": "2022-02-27T07:38:11.759499Z",
          "iopub.status.idle": "2022-02-27T07:38:11.767823Z",
          "shell.execute_reply.started": "2022-02-27T07:38:11.759459Z",
          "shell.execute_reply": "2022-02-27T07:38:11.766978Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rcnn = RCNN().to(device)\n",
        "criterion = rcnn.calc_loss\n",
        "optimizer = optim.SGD(rcnn.parameters(), lr=1e-3)\n",
        "n_epochs = 5\n",
        "log = Report(n_epochs) #records the metrics as report, can be used to plot later"
      ],
      "metadata": {
        "id": "JIobqz0W9I6s",
        "execution": {
          "iopub.status.busy": "2022-02-27T07:38:11.769286Z",
          "iopub.execute_input": "2022-02-27T07:38:11.769761Z",
          "iopub.status.idle": "2022-02-27T07:38:11.905027Z",
          "shell.execute_reply.started": "2022-02-27T07:38:11.769726Z",
          "shell.execute_reply": "2022-02-27T07:38:11.904255Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loc_loss: loss on classification\n",
        "# regr_loss: loss on regression\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "\n",
        "    _n = len(train_loader)\n",
        "    for ix, inputs in enumerate(train_loader):\n",
        "        loss, loc_loss, regr_loss, accs = train_batch(inputs, rcnn,\n",
        "                                                      optimizer, criterion)\n",
        "        pos = (epoch + (ix+1)/_n)\n",
        "        log.record(pos, trn_loss=loss.item(), trn_loc_loss=loc_loss,\n",
        "                   trn_regr_loss=regr_loss,\n",
        "                   trn_acc=accs.mean(), end='\\r')\n",
        "\n",
        "    _n = len(test_loader)\n",
        "    for ix,inputs in enumerate(test_loader):\n",
        "        _clss, _deltas, loss, \\\n",
        "        loc_loss, regr_loss, accs = validate_batch(inputs,\n",
        "                                                rcnn, criterion)\n",
        "        pos = (epoch + (ix+1)/_n)\n",
        "        log.record(pos, val_loss=loss.item(), val_loc_loss=loc_loss,\n",
        "                val_regr_loss=regr_loss,\n",
        "                val_acc=accs.mean(), end='\\r')\n",
        "\n",
        "# Plotting training and validation metrics\n"
      ],
      "metadata": {
        "id": "hReU_vnH9Kk0",
        "_kg_hide-input": false,
        "_kg_hide-output": true,
        "execution": {
          "iopub.status.busy": "2022-02-27T07:38:11.906695Z",
          "iopub.execute_input": "2022-02-27T07:38:11.906987Z",
          "iopub.status.idle": "2022-02-27T07:45:56.696269Z",
          "shell.execute_reply.started": "2022-02-27T07:38:11.906938Z",
          "shell.execute_reply": "2022-02-27T07:45:56.694962Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "log.plot_epochs('trn_loss,val_loss'.split(','))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-27T07:45:56.699203Z",
          "iopub.execute_input": "2022-02-27T07:45:56.699488Z",
          "iopub.status.idle": "2022-02-27T07:45:57.070685Z",
          "shell.execute_reply.started": "2022-02-27T07:45:56.699444Z",
          "shell.execute_reply": "2022-02-27T07:45:57.068323Z"
        },
        "trusted": true,
        "id": "UzD8m4CLDP6A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "log.plot_epochs('trn_acc,val_acc'.split(','))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-27T07:45:57.074913Z",
          "iopub.execute_input": "2022-02-27T07:45:57.076951Z",
          "iopub.status.idle": "2022-02-27T07:45:57.434745Z",
          "shell.execute_reply.started": "2022-02-27T07:45:57.076912Z",
          "shell.execute_reply": "2022-02-27T07:45:57.434072Z"
        },
        "trusted": true,
        "id": "3xGYYyeKDP6A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2 style=\"background-color:#EEEEEE; color:#222831; text-align:left; font-family:Georgia\">NMS (Non-max suppression)</h2>\n",
        "\n",
        "- Non-max refers to the boxes that do not contain the highest probability of containing an object, and suppression refers to us discarding those boxes that do not contain the highest probabilities of containing an object. In non-max suppression, we identify the bounding box that has the highest probability and discard all the other bounding boxes that have an IoU greater than a certain threshold with the box containing the highest probability of containing an object.\n",
        "- Using non-max suppression nms to eliminate near-duplicate bounding boxes: pairs of boxes that have an IoU greater than 0.05 are considered duplicates in this case. Among the duplicated boxes, we pick that box with the highest confidence and discard the rest"
      ],
      "metadata": {
        "id": "_oCvljjPDP6G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_predictions(filename, show_output=True):\n",
        "    img = np.array(cv2.imread(filename, 1)[...,::-1])\n",
        "    candidates = extract_candidates(img)\n",
        "    candidates = [(x,y,x+w,y+h) for x,y,w,h in candidates]\n",
        "    input = []\n",
        "    for candidate in candidates:\n",
        "        x,y,X,Y = candidate\n",
        "        crop = cv2.resize(img[y:Y,x:X], (224,224))\n",
        "        input.append(preprocess_image(crop/255.)[None])\n",
        "    input = torch.cat(input).to(device)  #Concatenates the given sequence of seq tensors in the given dimension\n",
        "    with torch.no_grad():\n",
        "        rcnn.eval()\n",
        "        probs, deltas = rcnn(input)\n",
        "        print(\"Shape of probs\", np.shape(probs))\n",
        "        print(\"Shape of deltas\", np.shape(deltas))\n",
        "        probs = torch.nn.functional.softmax(probs, -1)  # probability is put between 0 and 1 by doing a softmax\n",
        "        confs, clss = torch.max(probs, -1) #-1 is the dimension in which the max (reduction) must happen -> confs tells me the confidence and clss tells me the class where it has found the max probs\n",
        "        print(\"Shape of confs\", np.shape(confs))\n",
        "        print(\"Shape of clss\", np.shape(clss))\n",
        "    candidates = np.array(candidates)\n",
        "    confs, clss, probs, deltas = [tensor.detach().cpu().numpy() for tensor in [confs, clss, probs, deltas]]\n",
        "\n",
        "    ixs = clss!=background_class\n",
        "    confs, clss, probs, deltas, candidates = [tensor[ixs] for tensor in [confs, clss, probs, deltas, candidates]]\n",
        "    bbs = (candidates + deltas).astype(np.uint16)\n",
        "    ixs = nms(torch.tensor(bbs.astype(np.float32)), torch.tensor(confs), 0.05)\n",
        "    confs, clss, probs, deltas, candidates, bbs = [tensor[ixs] for tensor in [confs, clss, probs, deltas, candidates, bbs]]\n",
        "    if len(ixs) == 1:\n",
        "        # condition that even after nms, many ixs come for the bbox\n",
        "        confs, clss, probs, deltas, candidates, bbs = [tensor[None] for tensor in [confs, clss, probs, deltas, candidates, bbs]]\n",
        "    if len(confs) == 0 and not show_output:\n",
        "        return (0,0,224,224), 'background', 0\n",
        "    if len(confs) > 0:\n",
        "        best_pred = np.argmax(confs)  # index\n",
        "        best_conf = np.max(confs)\n",
        "        best_bb = bbs[best_pred]\n",
        "        x,y,X,Y = best_bb\n",
        "    _, ax = plt.subplots(1, 2, figsize=(20,10))\n",
        "    show(img, ax=ax[0])\n",
        "    ax[0].grid(False)\n",
        "    ax[0].set_title('Original image')\n",
        "    if len(confs) == 0:\n",
        "        ax[1].imshow(img)\n",
        "        ax[1].set_title('No objects')\n",
        "        plt.show()\n",
        "        return\n",
        "    ax[1].set_title('Predicted object: ' + target2label[clss[best_pred]])\n",
        "    show(img, bbs=bbs.tolist(), texts=[target2label[c] for c in clss.tolist()], ax=ax[1])\n",
        "    plt.show()\n",
        "    return (x,y,X,Y),target2label[clss[best_pred]],best_conf"
      ],
      "metadata": {
        "id": "qksBAgNJ9NhK",
        "execution": {
          "iopub.status.busy": "2022-02-27T07:47:01.221226Z",
          "iopub.execute_input": "2022-02-27T07:47:01.22155Z",
          "iopub.status.idle": "2022-02-27T07:47:01.239411Z",
          "shell.execute_reply.started": "2022-02-27T07:47:01.221518Z",
          "shell.execute_reply": "2022-02-27T07:47:01.238737Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image, crops, bbs, labels, deltas, gtbbs, fpath = test_ds[6]\n",
        "test_predictions(fpath)"
      ],
      "metadata": {
        "id": "0qpUkdh4E_zl",
        "execution": {
          "iopub.status.busy": "2022-02-27T07:47:01.974064Z",
          "iopub.execute_input": "2022-02-27T07:47:01.974646Z",
          "iopub.status.idle": "2022-02-27T07:47:03.288289Z",
          "shell.execute_reply.started": "2022-02-27T07:47:01.974609Z",
          "shell.execute_reply": "2022-02-27T07:47:03.2876Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}