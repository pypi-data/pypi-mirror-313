import dataclasses
import datetime as dt
import functools
import io
import logging
import os
import pathlib
import re
import typing
import uuid
from typing import Optional
from urllib.parse import urlparse

import chevron
import pydantic
from google.cloud import storage
from google.cloud.storage.fileio import BlobReader
from google.oauth2.credentials import Credentials

from tinybird.ingest.bigquery_dag import bigquery_dag_template
from tinybird.ingest.cdk_utils import CDK_IMAGE_REGISTRY, as_jinja_template
from tinybird.ingest.snowflake_dag import snowflake_dag_template

LOG_LEVEL = logging.DEBUG if os.environ.get("DEBUG") else logging.INFO

logger = logging.getLogger(__file__)
log_formatter = logging.Formatter("[%(asctime)s] %(levelname)s: %(message)s")
log_handler = logging.StreamHandler()
log_handler.setFormatter(log_formatter)
logger.addHandler(log_handler)
logger.setLevel(LOG_LEVEL)
logger.propagate = False

DAGS_DIR = "dags"
DAG_ID_PATTERN = re.compile(r"([A-Za-z0-9-]+)_(t_[A-Za-z0-9]+)")
CDK_DAG_FILE_KEY_PATTERN = re.compile(rf"{DAGS_DIR}/([a-z0-9-]+)/(t_[a-z0-9-]+)_dag.py")
CDK_DAG_ID_PATTERN = re.compile(r"dag_id=\"([A-Za-z0-9-]+_t_[A-Za-z0-9]+)\"")
CDK_CONTAINER_IMAGE_VERSION_PATTERN = re.compile(r"image=\'[\w/.-]+/cdk:(\w.+)\'")
CDK_SCHEDULE_INTERVAL_PATTERN = re.compile(r'schedule_interval="(.*)"')
CDK_START_DATE_PATTERN = re.compile(r'start_date=datetime.datetime.strptime\("(.*)", "%Y-%m-%d %H:%M:%S"\),')
CDK_POOL_PATTERN = re.compile(r'pool="(.+)_pool",')
# 0.9.1 version didn't encode the SA key & query in base64 so we need a different pattern.
CDK_SA_KEY_PATTERN_0_9_1 = re.compile(r"'GOOGLE_APPLICATION_CREDENTIALS_JSON': json.dumps\((.*)\),", re.S)
CDK_QUERY_PATTERN_0_9_1 = re.compile(r"'SQL_QUERY': '''(.*)''',")

DEFAULT_BACKUP_DIR = "dag_backups"
CACHE_DIR = "/tmp/cdk_tool"


class NotACDKDagFile(Exception):
    pass


@dataclasses.dataclass(frozen=True)
class ComposerCDKDAGFileKey:
    workspace_id: str
    datasource_id: str

    def __str__(self) -> str:
        return f"{DAGS_DIR}/{self.workspace_id}/{self.datasource_id}_dag.py"


@dataclasses.dataclass(frozen=True)
class ComposerCDKDAGFile:
    workspace_id: str
    datasource_id: str
    updated_at: dt.datetime

    @property
    def key(self) -> ComposerCDKDAGFileKey:
        return ComposerCDKDAGFileKey(self.workspace_id, self.datasource_id)

    @property
    def dag_id(self) -> str:
        return f"{self.workspace_id}_{self.datasource_id}"


class ComposerCDKDAGConfig(pydantic.BaseModel):
    workspace_id: str
    datasource_id: str
    dag_id: str
    container_version: str
    updated_at: dt.datetime
    schedule_interval: str
    start_date: str
    kind: str
    tb_endpoint: str
    tb_token: str
    export_bucket: str
    sa_key: str
    mode: str
    query: str
    query_autogenerated: Optional[bool]
    row_limit: Optional[int]
    logs_endpoint: Optional[str]
    logs_token: Optional[str]
    logs_datasource: Optional[str]
    pool_id: Optional[str]
    context_workspace_id: Optional[str]
    context_datasource_id: str

    @property
    def dag_file(self) -> ComposerCDKDAGFile:
        return ComposerCDKDAGFile(self.workspace_id, self.datasource_id, self.updated_at)


class BigQueryDAGConfig(ComposerCDKDAGConfig):
    pass


class SnowflakeExternalDatasourceSettings(pydantic.BaseModel):
    account: str
    user: str
    password: str
    role: str
    warehouse: str
    database: str
    schema_: str = pydantic.Field(alias="schema")
    stage: str


class SnowflakeDAGConfig(ComposerCDKDAGConfig):
    sfk_settings: SnowflakeExternalDatasourceSettings


class ComposerDAGFileBackup(pydantic.BaseModel):
    timestamp: dt.datetime
    workspace_id: str
    datasource_id: str
    id: str
    dag_file_updated_at: dt.datetime
    dag_file_content: str


SENTINEL_BACKUP = ComposerDAGFileBackup(
    timestamp=dt.datetime(2000, 1, 1, tzinfo=dt.UTC),
    workspace_id="",
    datasource_id="",
    id="sentinel",
    dag_file_updated_at=dt.datetime(2000, 1, 1, tzinfo=dt.UTC),
    dag_file_content="",
)


def _parse_cdk_dag_file_key(key: str) -> tuple[str, str]:
    match = re.search(CDK_DAG_FILE_KEY_PATTERN, key)
    if not match:
        raise NotACDKDagFile(key)
    return (match.group(1), match.group(2))


class PatternNotFound(Exception):
    pass


def extract(needle: str | re.Pattern[str], haystack: str) -> str:
    match = re.search(needle, haystack)
    if not match:
        raise PatternNotFound(needle)
    return match.group(1)


def maybe_extract(needle: str | re.Pattern[str], haystack: str) -> Optional[str]:
    match = re.search(needle, haystack)
    if not match:
        return None
    return match.group(1)


def extract_dag_env_var(key: str, dag_file: str) -> str:
    return extract(rf"'{key}': \"(.*)\",", dag_file)


def extract_base64_encoded_dag_env_var(key: str, dag_file: str) -> str:
    # Adjust the pattern to match the structure of the string in the dag_file
    pattern = rf"'{key}': base64\.b64decode\(\"([^\"]+)\"\)\.decode\('ascii'\)"

    # Search for the pattern in the dag_file string
    match = re.search(pattern, dag_file)

    # Return the captured group if a match is found, otherwise return an empty string
    if match:
        return match.group(1)
    else:
        return ""


def maybe_extract_dag_env_var(key: str, dag_file: str) -> Optional[str]:
    return maybe_extract(f"'{key}': \"(.*)\",", dag_file)


def _cdk_dag_file_from_blob(blob: storage.Blob) -> ComposerCDKDAGFile:
    workspace_id, datasoure_id = _parse_cdk_dag_file_key(blob.name)
    return ComposerCDKDAGFile(
        workspace_id=workspace_id,
        datasource_id=datasoure_id,
        updated_at=blob.updated,
    )


class NoDAGBackupFound(Exception):
    pass


class StaleCachedConfig(Exception):
    pass


class ComposerCDKDAGFileReader:
    def __init__(self, credentials: Credentials, bucket_name: str) -> None:
        self._credentials = credentials
        self._bucket_name = bucket_name

    @functools.cached_property
    def _storage_client(self) -> storage.Client:
        return storage.Client(credentials=self._credentials)

    def list_dag_files(self, workspace_id: Optional[str] = None) -> typing.Iterable[ComposerCDKDAGFile]:
        prefix = os.path.join(DAGS_DIR, workspace_id or "")  # Ensure we have a backslash
        logger.debug("Fetching all blobs in bucket '%s' under prefix '%s'...", self._bucket_name, prefix)
        blobs = self._storage_client.list_blobs(self._bucket_name, prefix=prefix)
        for blob in blobs:
            try:
                yield _cdk_dag_file_from_blob(blob)
            except NotACDKDagFile as err:
                logger.debug("Blob '%s' is not a CDK DAG file. Skipping...", err)
        logger.debug("%d blobs fetched", blobs.num_results)

    def get_dag_file(self, workspace_id: str, datasource_id: str) -> ComposerCDKDAGFile:
        key = ComposerCDKDAGFileKey(workspace_id, datasource_id)
        blob = self._get_blob(key)
        return _cdk_dag_file_from_blob(blob)

    def read_dag_file(self, dag_file: ComposerCDKDAGFile) -> io.TextIOWrapper:
        return self._download_blob(dag_file.key)

    def _get_config_from_cache(self, dag_file: ComposerCDKDAGFile) -> ComposerCDKDAGConfig:
        cached_file_path = pathlib.Path(CACHE_DIR, self._bucket_name, dag_file.workspace_id, dag_file.datasource_id)
        if not cached_file_path.exists():
            logger.debug("Config for DAG '%s' not found in cache or cache is stale.", dag_file.dag_id)
            raise FileNotFoundError(cached_file_path)
        logger.debug("Cached config found for DAG '%s'.", dag_file.dag_id)
        serialized_config = cached_file_path.read_bytes()
        base_config = ComposerCDKDAGConfig.model_validate_json(serialized_config)
        if dag_file.updated_at > base_config.updated_at:
            logger.debug("There is a newer config version for DAG '%s'.", dag_file.dag_id)
            raise StaleCachedConfig()
        if "snowflake" == base_config.kind:
            return SnowflakeDAGConfig.model_validate_json(serialized_config)
        elif "bigquery" == base_config.kind:
            return BigQueryDAGConfig.model_validate_json(serialized_config)
        else:
            raise ValueError(base_config.kind)

    def _write_config_to_cache(self, config: ComposerCDKDAGConfig) -> None:
        logger.debug("Cached config found for DAG '%s_%s'.", config.workspace_id, config.datasource_id)
        cached_file_path = pathlib.Path(CACHE_DIR, self._bucket_name, config.workspace_id, config.datasource_id)
        cached_file_path.parent.mkdir(exist_ok=True, parents=True)
        cached_file_path.write_text(config.model_dump_json(by_alias=True))

    def get_dag_config(self, dag_file: ComposerCDKDAGFile) -> ComposerCDKDAGConfig:
        try:
            return self._get_config_from_cache(dag_file)
        except (FileNotFoundError, StaleCachedConfig):
            pass
        file_content = self._download_blob(dag_file.key).read()
        cdk_version = extract(CDK_CONTAINER_IMAGE_VERSION_PATTERN, file_content)
        kind = extract_dag_env_var("CONNECTOR", file_content)
        config_dict = self._extract_basic_config_from_dag_file(file_content)
        config_dict["workspace_id"] = dag_file.workspace_id
        config_dict["datasource_id"] = dag_file.datasource_id
        config_dict["updated_at"] = dag_file.updated_at
        dag_config: ComposerCDKDAGConfig  # Make mypy happy
        if kind == "snowflake":
            sfk_settings = self._extract_snowflake_sessings(cdk_version, file_content)
            dag_config = SnowflakeDAGConfig.model_validate(dict(**config_dict, sfk_settings=sfk_settings))
        elif kind == "bigquery":
            dag_config = BigQueryDAGConfig.model_validate(config_dict)
        else:
            raise ValueError(f"Unknown external datasource kind: '{kind}'")
        self._write_config_to_cache(dag_config)
        return dag_config

    def _extract_basic_config_from_dag_file(self, file_content: str) -> dict[str, typing.Any]:
        cdk_version = extract(CDK_CONTAINER_IMAGE_VERSION_PATTERN, file_content)
        # TODO: Remove this once we don't have 9.1 anymore
        if cdk_version == "0.9.1":
            sa_key = extract(CDK_SA_KEY_PATTERN_0_9_1, file_content)
            query = extract(CDK_QUERY_PATTERN_0_9_1, file_content)
        else:
            sa_key = extract_dag_env_var("GOOGLE_APPLICATION_CREDENTIALS_JSON", file_content)
            query = extract_dag_env_var("SQL_QUERY", file_content)

        # TODO: Remove this once we don't have this versions anymore
        if cdk_version in ("0.10.1", "0.9.1"):
            context_datasource_id = extract_dag_env_var("DATA_SOURCE", file_content)
        else:
            context_datasource_id = extract_dag_env_var("TB_DATASOURCE_ID", file_content)
        return dict(
            dag_id=extract(CDK_DAG_ID_PATTERN, file_content),
            container_version=cdk_version,
            schedule_interval=extract(CDK_SCHEDULE_INTERVAL_PATTERN, file_content),
            start_date=extract(CDK_START_DATE_PATTERN, file_content),
            tb_endpoint=extract_dag_env_var("TB_CDK_ENDPOINT", file_content),
            pool_id=maybe_extract(CDK_POOL_PATTERN, file_content),
            kind=extract_dag_env_var("CONNECTOR", file_content),
            tb_token=extract_dag_env_var("TB_CDK_TOKEN", file_content),
            export_bucket=extract_dag_env_var("GCS_BUCKET", file_content),
            sa_key=sa_key,
            mode=extract_dag_env_var("COMMAND", file_content),
            logs_endpoint=maybe_extract_dag_env_var("TB_LOGS_ENDPOINT", file_content),
            logs_token=maybe_extract_dag_env_var("TB_LOGS_TOKEN", file_content),
            logs_datasource=maybe_extract_dag_env_var("TB_LOGS_DATASOURCE", file_content),
            query=query,
            query_autogenerated=maybe_extract_dag_env_var("SQL_QUERY_AUTOGENERATED", file_content)
            or None,  # Empty string to None
            row_limit=maybe_extract_dag_env_var("ROW_LIMIT", file_content),
            context_workspace_id=maybe_extract_dag_env_var("TB_WORKSPACE_ID", file_content),
            context_datasource_id=context_datasource_id,
        )

    def _extract_snowflake_sessings(self, cdk_version: str, file_content: str) -> SnowflakeExternalDatasourceSettings:
        if cdk_version in ("0.11.2", "0.16.5"):
            account = extract_dag_env_var("SF_ACCOUNT", file_content)
            user = extract_dag_env_var("SF_USER", file_content)
            password = extract_dag_env_var("SF_PWD", file_content)
        else:
            account = extract_base64_encoded_dag_env_var("SF_ACCOUNT", file_content)
            user = extract_base64_encoded_dag_env_var("SF_USER", file_content)
            password = extract_base64_encoded_dag_env_var("SF_PWD", file_content)

        return SnowflakeExternalDatasourceSettings(
            account=account,
            user=user,
            password=password,
            role=extract_dag_env_var("SF_ROLE", file_content),
            warehouse=extract_dag_env_var("SF_WAREHOUSE", file_content),
            database=extract_dag_env_var("SF_DATABASE", file_content),
            schema=extract_dag_env_var("SF_SCHEMA", file_content),
            stage=extract_dag_env_var("SF_STAGE", file_content),
        )

    def check_dag_config(self, config: ComposerCDKDAGConfig) -> typing.Iterable[str]:
        dag_id = f"{config.workspace_id}_{config.datasource_id}"
        if config.dag_id != dag_id:
            yield f"DAG {dag_id}: Dagfile dag_id '{config.dag_id}' doesn't match the dag id implicit in the file path '{dag_id}'"

        if config.context_workspace_id and config.context_workspace_id != config.workspace_id:
            yield f"DAG {dag_id}: Context workspace id '{config.context_workspace_id}' doesn't match the workspace id in the path: {config.workspace_id}'"

        if config.context_datasource_id and config.context_datasource_id != config.datasource_id:
            yield f"DAG {dag_id}: Context datasource id '{config.context_datasource_id}' doesn't match the datasource id in the path: {config.datasource_id}'"

    def backup_dag_file(self, backup_dir: str, dag_file: ComposerCDKDAGFile) -> None:
        backup_time = dt.datetime.now(dt.UTC)
        latest_backup = self._get_latest_backup(backup_dir, dag_file.workspace_id, dag_file.datasource_id)
        if latest_backup.dag_file_updated_at == dag_file.updated_at:
            logger.info("Up-to-date backup for dag %s already exists. Skipping.", dag_file.dag_id)
            return
        backup = ComposerDAGFileBackup(
            timestamp=backup_time,
            workspace_id=dag_file.workspace_id,
            datasource_id=dag_file.datasource_id,
            id=uuid.uuid4().hex,
            dag_file_updated_at=dag_file.updated_at,
            dag_file_content=self._download_blob(dag_file.key).read(),
        )
        backup_dir = os.path.join(backup_dir, f"{dag_file.workspace_id}/{dag_file.datasource_id}")
        pathlib.Path(backup_dir).mkdir(exist_ok=True, parents=True)
        backup_file_path = f"{backup_dir}/{backup.id}"
        logger.debug("Storing DAG file backup into '%s'...", backup_file_path)
        with open(backup_file_path, "w+") as ostream:
            ostream.write(backup.model_dump_json(by_alias=True))

    def get_dag_backup(
        self, backup_dir: str, workspace_id: str, datasource_id: str, backup_id: str
    ) -> ComposerDAGFileBackup:
        backup_file = pathlib.Path(backup_dir, workspace_id, datasource_id, backup_id)
        if not backup_file.exists():
            raise NoDAGBackupFound(f"No DAG backup found for DAG {workspace_id}_{datasource_id} with id {backup_id}")
        return ComposerDAGFileBackup.model_validate_json(backup_file.read_bytes())

    def get_latest_dag_backup(self, backup_dir: str, workspace_id: str, datasource_id: str) -> ComposerDAGFileBackup:
        if SENTINEL_BACKUP == (backup := self._get_latest_backup(backup_dir, workspace_id, datasource_id)):
            raise NoDAGBackupFound(f"No DAG backup found for DAG {workspace_id}_{datasource_id}")
        return backup

    def list_backups(
        self, backup_dir: str, workspace_id: Optional[str] = None, datasource_id: Optional[str] = None
    ) -> typing.Iterable[ComposerDAGFileBackup]:
        if workspace_id and datasource_id:
            _backup_dir = pathlib.Path(backup_dir, workspace_id, datasource_id)
        elif workspace_id:
            _backup_dir = pathlib.Path(backup_dir, workspace_id)
        else:
            _backup_dir = pathlib.Path(backup_dir)

        for file in _backup_dir.glob("**/*"):
            if file.is_file():
                yield ComposerDAGFileBackup.model_validate_json(file.read_bytes())

    def render_updated_dag_file(
        self, config: ComposerCDKDAGConfig, version: str, envvars: Optional[dict] = None
    ) -> str:
        context = self._render_common_context(config)
        context["CDK_IMAGE"] = f"{CDK_IMAGE_REGISTRY}:{version}"

        custom_settings = {}
        if envvars:
            for k, v in envvars.items():
                custom_settings[k] = v

        match config:
            case SnowflakeDAGConfig(sfk_settings=settings):
                sfk_context = self._render_snowflake_settings(settings)
                context |= sfk_context
                context |= custom_settings
                return chevron.render(snowflake_dag_template, context)
            case BigQueryDAGConfig():
                context |= custom_settings
                return chevron.render(bigquery_dag_template, context)
            case _:
                raise TypeError(config.__class__.__name__)

    def _render_common_context(self, config: ComposerCDKDAGConfig) -> dict[str, typing.Any]:
        hostname = str(urlparse(config.tb_endpoint).netloc)
        return dict(
            ID=config.dag_id,
            CRON=config.schedule_interval,
            START_DATE=config.start_date,
            POOL_ID=config.pool_id or f"{config.workspace_id}_pool",
            TB_WORKSPACE_ID=config.context_workspace_id or config.workspace_id,
            TB_DATASOURCE_ID=config.context_datasource_id,
            TB_CDK_ENDPOINT=config.tb_endpoint,
            TB_TOKEN=config.tb_token,
            GCS_BUCKET=config.export_bucket,
            GCP_SA_KEY=config.sa_key,
            SERVICE=config.kind,
            MODE=config.mode,
            SQL_QUERY=config.query,
            SQL_QUERY_AUTOGENERATED=config.query_autogenerated or "False",
            ROW_LIMIT=config.row_limit or 50_000_000,
            ENVIRONMENT_TEMPLATE=as_jinja_template("var.value.environment"),
            SENTRY_CONN_TEMPLATE=as_jinja_template("conn.tb_sentry_dsn.get_uri()"),
            TB_LOGS_ENDPOINT_TEMPLATE=as_jinja_template(f"conn.get('logs-{hostname}').host"),
            TB_LOGS_TOKEN_TEMPLATE=as_jinja_template(f"conn.get('logs-{hostname}').password"),
            TB_LOGS_DATASOURCE_TEMPLATE=as_jinja_template(f"conn.get('logs-{hostname}').extra_dejson.datasource_id"),
        )

    def _render_snowflake_settings(self, settings: SnowflakeExternalDatasourceSettings) -> dict[str, typing.Any]:
        return dict(
            SF_ACCOUNT=settings.account,
            SF_USER=settings.user,
            SF_PWD=settings.password,
            SF_ROLE=settings.role,
            SF_WAREHOUSE=settings.warehouse,
            SF_DATABASE=settings.database,
            SF_SCHEMA=settings.schema_,
            SF_STAGE=settings.stage,
        )

    def overwrite_dag_file(self, dag_file: ComposerCDKDAGFile, content: str) -> None:
        key = ComposerCDKDAGFileKey(dag_file.workspace_id, dag_file.datasource_id)
        blob = self._get_blob(key)
        logger.info("Uploading DAG file to %s...", key)
        blob.upload_from_string(content)
        logger.info("DAG file uploaded")

    def _get_latest_backup(self, backup_dir: str, workspace_id: str, datasource_id: str) -> ComposerDAGFileBackup:
        _backup_dir = pathlib.Path(backup_dir, workspace_id, datasource_id)
        if not _backup_dir.exists():
            return SENTINEL_BACKUP
        latest_backup = SENTINEL_BACKUP
        for file in _backup_dir.iterdir():
            backup = ComposerDAGFileBackup.model_validate_json(file.read_bytes())
            if backup.timestamp > latest_backup.timestamp:
                latest_backup = backup
        return latest_backup

    def _get_blob(self, key: ComposerCDKDAGFileKey) -> storage.Blob:
        bucket = self._storage_client.get_bucket(self._bucket_name)
        if not (blob := bucket.get_blob(str(key))):
            raise FileNotFoundError(str(key))
        return blob

    def _download_blob(self, key: ComposerCDKDAGFileKey) -> io.TextIOWrapper:
        blob = self._get_blob(key)
        logger.debug("Downloading blob '%s'...", key)
        return io.TextIOWrapper(BlobReader(blob))
