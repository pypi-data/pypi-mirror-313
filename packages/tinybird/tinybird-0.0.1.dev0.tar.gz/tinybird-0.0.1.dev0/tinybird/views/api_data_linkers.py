import asyncio
import base64
from dataclasses import dataclass
from typing import Any, Dict, List, Optional, Tuple, Union

import googleapiclient.errors
import snowflake.connector.errors
from tornado.web import url

from tinybird.data_connector import DataConnector, DataConnectors, DataLinker
from tinybird.datasource import Datasource
from tinybird.ingest.cdk_utils import (
    CDKUtils,
    InvalidRole,
    QueryParsingFailed,
    get_field_names_from_query,
    get_gcs_bucket_uri,
    is_valid_cron_expression,
    normalize_cron_expression,
    validate_snowflake_role,
)
from tinybird.ingest.external_datasources.admin import (
    get_or_create_workspace_service_account,
    grant_bucket_write_permissions_to_account,
)
from tinybird.ingest.external_datasources.connector import (
    SNOWFLAKE_TINYBIRD_INTEGRATION_FORMAT,
    InvalidGCPCredentials,
    get_connector,
)
from tinybird.ingest.external_datasources.inspection import ExternalTableDatasource
from tinybird.limits import Limit
from tinybird.sql import parse_table_structure
from tinybird.tokens import AccessToken, scopes
from tinybird.user import User as Workspace
from tinybird.user import Users as Workspaces
from tinybird.views.api_data_connectors import APIDataConnectorsBase
from tinybird.views.api_errors.data_connectors import (
    DataConnectorsClientErrorBadRequest,
    DataConnectorsClientErrorNotFound,
)
from tinybird.views.api_errors.datasources import ClientErrorBadRequest
from tinybird.views.base import ApiHTTPError, BaseHandler, authenticated, check_rate_limit, with_scope


async def update_bigquery_linker(workspace, datasource, params):
    query = params.get("query", None)
    cron = params.get("cron", None)
    mode = params.get("mode", None)

    current_bigquery_settings = datasource.service_conf
    base64_query = base64.b64encode(query.encode("utf-8")).decode("ascii") if query is not None else None
    if (
        query is None
        and base64_query == current_bigquery_settings["SQL_QUERY"]
        and cron is None
        and cron == current_bigquery_settings["CRON"]
        and mode is None
        and mode == current_bigquery_settings["MODE"]
    ):
        return

    final_cron = cron if cron is not None else current_bigquery_settings["CRON"]
    final_mode = mode if mode is not None else current_bigquery_settings["MODE"]
    final_query = query if query else current_bigquery_settings["SQL_QUERY"]

    final_query_b64 = (
        base64.b64encode(query.encode("utf-8")).decode("ascii") if query else current_bigquery_settings["SQL_QUERY"]
    )
    ds_service_conf = {"CRON": final_cron, "MODE": final_mode, "SQL_QUERY": final_query_b64}
    datasource.service_conf = ds_service_conf
    workspace.update_datasource(datasource)

    token = await asyncio.to_thread(get_bigquery_token, workspace, datasource.id)
    await update_dag(DataConnectors.BIGQUERY, workspace, datasource, token.visible_token, ds_service_conf)
    # save after the dag is created
    await Workspaces.update_datasource_async(workspace, datasource)
    return {"CRON": final_cron, "MODE": final_mode, "SQL_QUERY": final_query}


def get_bigquery_token(workspace: Workspace, datasource_id: str) -> AccessToken:
    tokens = Workspaces.get_access_tokens_for_resource(workspace, datasource_id, scopes.DATASOURCES_CREATE)
    if tokens:
        return tokens[0]

    token = Workspaces.get_access_token_for_scope(workspace, scopes.DATASOURCES_CREATE)
    if not token:
        raise ApiHTTPError.from_request_error(DataConnectorsClientErrorNotFound.no_data_connector_token_found())
    return token


def encode_private_members(conf: Dict[str, Union[bool, str, None]]) -> Dict[str, Union[bool, str, None]]:
    for key in ("SF_ACCOUNT", "SF_USER", "SF_PWD", "SQL_QUERY"):
        if key in conf and (conf_value := conf[key]) is not None and type(conf_value) == str:  # noqa: E721
            assert conf_value
            conf[key] = base64.b64encode(conf_value.encode("utf-8")).decode("ascii")

    return conf


def prepare_snowflake_dag_params(workspace: Workspace, data_connector: DataConnector, linker: DataLinker):
    linker_settings = linker.settings
    sf_database, sf_schema, _ = linker_settings["external_data_source"].split(".")

    ds_service_conf = {
        "CRON": linker_settings["cron"],
        "MODE": linker_settings["mode"],
        "SQL_QUERY": linker_settings["query"],
        "SQL_QUERY_AUTOGENERATED": linker_settings["query_autogenerated"],
        "SF_ACCOUNT": data_connector.settings["account"],
        "SF_USER": data_connector.settings["username"],
        "SF_PWD": data_connector.settings["password"],
        "SF_ROLE": data_connector.settings["role"],
        "SF_WAREHOUSE": data_connector.settings["warehouse"],
        "SF_DATABASE": sf_database,
        "SF_SCHEMA": sf_schema,
        "TB_WORKSPACE_ID": workspace.id,
        "SF_STAGE": linker_settings["stage"],
    }

    encode_private_members(ds_service_conf)
    return ds_service_conf


dag_params_builders = {DataConnectors.SNOWFLAKE: prepare_snowflake_dag_params}


async def update_generic_linker(
    service: DataConnectors,
    workspace: Workspace,
    datasource: Datasource,
    data_connector: DataConnector,
    linker: DataLinker,
    linker_params: Dict[str, str],
) -> Dict[str, Any]:
    new_linker_settings: Dict[str, Any] = {}
    has_changed = False
    for setting_key, old_value in linker.settings.items():
        tmp = linker_params.get(setting_key, None)
        new_value = tmp if tmp is not None else old_value

        new_linker_settings[setting_key] = new_value
        has_changed |= old_value != new_value

    if has_changed:
        linker.update_settings(new_linker_settings)
        ds_service_conf = dag_params_builders[service](workspace, data_connector, linker)
        await update_dag(service, workspace, datasource, new_linker_settings["tb_token"], ds_service_conf)
        linker = await DataLinker.update_settings_async(linker, new_linker_settings)

    return new_linker_settings


async def update_dag(
    service: str, workspace: Workspace, datasource: Datasource, token: str, ds_service_conf: Dict[str, str]
) -> None:
    assert isinstance(workspace.cdk_gcp_service_account, dict)
    gcp_service_account_key = workspace.cdk_gcp_service_account["key"]
    dag = CDKUtils.prepare_dag(
        service,
        workspace,
        datasource.id,
        token,
        {**ds_service_conf, "GCP_SA_KEY": base64.b64encode(gcp_service_account_key.encode("utf-8")).decode("ascii")},
    )
    await CDKUtils.upload_dag(dag, workspace.id, datasource.id)


async def create_snowflake_stage(env: Dict[str, str]) -> Dict[str, str]:
    gcs_bucket_uri = get_gcs_bucket_uri(env["TB_WORKSPACE_ID"])
    sf_env = env.copy()

    try:
        cdk_connector = await get_connector(DataConnectors.SNOWFLAKE, sf_env)
    except InvalidGCPCredentials as err:
        # If we get here something has gone terribly wrong and the credentials might be invalid/corrupted
        raise ApiHTTPError(401, "Invalid credentials") from err

    try:
        custom_stage = sf_env["SF_STAGE"]
        custom_integration = sf_env["SF_INTEGRATION"]
        integration = (
            custom_integration
            if custom_integration
            else SNOWFLAKE_TINYBIRD_INTEGRATION_FORMAT.format(role=sf_env["SF_ROLE"])
        )
        res = await cdk_connector.create_stage(gcs_bucket_uri, integration, custom_stage)
        sf_env["SF_STAGE"] = res["stage"]
        assert CDKUtils.gcs_export_bucket
        grant_bucket_write_permissions_to_account(
            CDKUtils.get_credentials_provider(),
            CDKUtils.get_project_id(),
            CDKUtils.gcs_export_bucket,
            res["gcp_account"],
        )
    except snowflake.connector.errors.Error as e:
        if e.raw_msg is not None:
            msg = e.raw_msg
            if "access control" in msg.lower() or "insufficient privileges" in msg.lower():
                msg += f'.\nTry granting access to the database for your role executing \'grant create stage on all schemas in database {sf_env["SF_DATABASE"]} to role {sf_env["SF_ROLE"]};\''
            raise ApiHTTPError(403, msg) from e
        else:
            raise ApiHTTPError(403, "Error connecting to Snowflake")
    except googleapiclient.errors.HttpError as e:
        raise ApiHTTPError(e.status_code, e.reason) from e
    except InvalidGCPCredentials as e:
        raise ApiHTTPError(401, "Invalid credentials") from e
    finally:
        cdk_connector.shutdown()

    return sf_env


@dataclass
class ConnectorLinkerParams:
    mode: Optional[str]
    query: Optional[str]
    cron: Optional[str]
    stage: Optional[str]
    external_data_source: Optional[str]
    ingest_now: bool = False
    query_autogenerated: bool = False


async def setup_connector_params(
    workspace: Workspace, handler: BaseHandler, schema: str, service: str, data_connector: Optional[DataConnector]
) -> ConnectorLinkerParams:
    conn_params = ConnectorLinkerParams(None, None, None, None, None)

    conn_params.cron = handler.get_argument("cron", None)
    if conn_params.cron is not None and not is_valid_cron_expression(conn_params.cron):
        raise ApiHTTPError.from_request_error(ClientErrorBadRequest.external_datasource_invalid_cron())

    conn_params.cron = normalize_cron_expression(conn_params.cron)

    conn_params.mode = handler.get_argument("mode", "replace").lower()
    if conn_params.mode not in ["append", "replace"]:
        raise ApiHTTPError.from_request_error(ClientErrorBadRequest.service_invalid_mode())

    conn_params.external_data_source = handler.get_argument("external_data_source", None)
    conn_params.query = handler.get_argument("query", None)
    if service == DataConnectors.BIGQUERY and conn_params.external_data_source is None and conn_params.query is None:
        raise ApiHTTPError.from_request_error(ClientErrorBadRequest.external_datasource_or_query_required())
    elif service != DataConnectors.BIGQUERY and conn_params.external_data_source is None:
        raise ApiHTTPError.from_request_error(ClientErrorBadRequest.external_datasource_required())

    if conn_params.external_data_source is not None and len(conn_params.external_data_source.split(".")) < 2:
        raise ApiHTTPError.from_request_error(ClientErrorBadRequest.external_datasource_required())

    if conn_params.query is None:
        assert conn_params.external_data_source
        if service == DataConnectors.SNOWFLAKE and data_connector:
            cdk_env = {
                "SF_ACCOUNT": data_connector.settings["account"],
                "SF_USER": data_connector.settings["username"],
                "SF_PWD": data_connector.settings["password"],
                "SF_ROLE": data_connector.settings["role"],
                "SF_WAREHOUSE": data_connector.settings["warehouse"],
                "SF_DATABASE": "",
                "SF_SCHEMA": "",
            }
        elif service == DataConnectors.BIGQUERY:
            account_info = await get_or_create_workspace_service_account(workspace)
            cdk_env = {"GOOGLE_APPLICATION_CREDENTIALS_JSON": account_info["key"]}
        else:
            raise NotImplementedError(f'Connector not implemented for service "{service}"')

        try:
            cdk_connector = await get_connector(service, cdk_env)
        except InvalidGCPCredentials as err:
            # If we get here something has gone terribly wrong and the credentials might be invalid/corrupted
            raise ApiHTTPError(401, "Invalid credentials") from err

        table = ExternalTableDatasource(cdk_connector, conn_params.external_data_source.rsplit(".", 2))
        try:
            conn_params.query = await table.get_extraction_query()
            conn_params.query_autogenerated = True
        except Exception as e:
            raise ApiHTTPError(403, str(e)) from e

    try:
        query_field_names = get_field_names_from_query(conn_params.query, service)
    except QueryParsingFailed as err:
        raise ApiHTTPError.from_request_error(ClientErrorBadRequest.invalid_sql_query(message=err.args[0]))
    datasource_columns = [item["name"] for item in parse_table_structure(schema)]

    datasource_columns_lower = set([x.lower() for x in datasource_columns])
    query_field_names_lower = set([x.lower() for x in query_field_names])
    if not datasource_columns_lower.issuperset(query_field_names_lower):
        msg = f'''Provided query fields "{query_field_names}" are not a subset of datasource schema "{datasource_columns}"'''
        raise ApiHTTPError.from_request_error(ClientErrorBadRequest.invalid_sql_query(message=msg))

    conn_params.ingest_now = handler.get_argument("ingest_now", "true").lower() == "true"

    # Disable the ingestion now in case this is a one off
    if conn_params.cron == "@once" and conn_params.ingest_now:
        conn_params.ingest_now = False

    return conn_params


async def prepare_connector_service(
    handler: BaseHandler, workspace: Workspace, service: str, data_connector: Optional[DataConnector], schema: str
) -> Tuple[Dict[str, Union[bool, str, None]], ConnectorLinkerParams]:
    conn_params = await setup_connector_params(workspace, handler, schema, service, data_connector)

    # Make mypy happy
    assert conn_params.mode
    assert conn_params.query

    ds_service_conf = {
        "CRON": conn_params.cron,
        "MODE": conn_params.mode,
        "SQL_QUERY": conn_params.query,
        "SQL_QUERY_AUTOGENERATED": conn_params.query_autogenerated,
    }

    if conn_params.external_data_source:
        ds_service_conf["EXTERNAL_DATA_SOURCE"] = conn_params.external_data_source

    if data_connector and data_connector.service == DataConnectors.SNOWFLAKE:
        # We shouldn't have an invalid role at this point because we're sanitizing them when saving
        # the Data Connector itself. However, we'd rather be safe than sorry in case the Redis DB
        # is modified, because this would end up injecting something into the user's Snowflake setup.
        assert conn_params.external_data_source
        try:
            validate_snowflake_role(data_connector.settings["role"])
        except InvalidRole as e:
            raise ApiHTTPError(403, e.message) from e

        sf_database, sf_schema, _ = conn_params.external_data_source.split(".")

        sf_env = {
            "SF_ACCOUNT": data_connector.settings["account"],
            "SF_USER": data_connector.settings["username"],
            "SF_PWD": data_connector.settings["password"],
            "SF_ROLE": data_connector.settings["role"],
            "SF_WAREHOUSE": data_connector.settings["warehouse"],
            "SF_STAGE": data_connector.settings.get("stage", None),
            "SF_INTEGRATION": data_connector.settings.get("integration", None),
            "SF_DATABASE": sf_database,
            "SF_SCHEMA": sf_schema,
            "TB_WORKSPACE_ID": workspace.id,
        }

        sf_env = await create_snowflake_stage(sf_env)
        conn_params.stage = sf_env["SF_STAGE"]
        ds_service_conf.update(sf_env)

    # We need to encode using base64 the private details for the DAG. We can't do this sooner
    # because the CDK only accepts them in plain text for the create_stage step.
    encode_private_members(ds_service_conf)

    return ds_service_conf, conn_params


async def add_cdk_data_linker(
    datasource: Datasource,
    data_connector: Optional[DataConnector],
    conn_params: ConnectorLinkerParams,
    service: str,
    workspace: Workspace,
) -> str:
    previous_data_linker = None

    if service in [DataConnectors.SNOWFLAKE]:
        try:
            previous_data_linker = DataLinker.get_by_datasource_id(datasource.id)
        except Exception:
            pass
        else:
            if previous_data_linker.service != service:
                raise ApiHTTPError(400, f"Data source {datasource.name} is already linked to a connector")

    connector_name: str = (
        data_connector.name if (data_connector is not None and data_connector.name is not None) else service  # type: ignore[union-attr]
    )
    token = await Workspaces.add_data_source_connector_token_async(
        user_id=workspace.id, connector_name=connector_name, datasource=datasource
    )

    if data_connector and service in [DataConnectors.SNOWFLAKE]:
        linker_settings = {
            "tb_token": token,
            "external_data_source": conn_params.external_data_source,
            "mode": conn_params.mode,
            "query": conn_params.query,
            "query_autogenerated": conn_params.query_autogenerated,
            "cron": conn_params.cron,
            "ingest_now": conn_params.ingest_now,
            "stage": conn_params.stage,
        }

        await DataLinker.add_linker(
            data_connector=data_connector, datasource=datasource, workspace=workspace, settings=linker_settings
        )

        if previous_data_linker:
            DataLinker._delete(previous_data_linker.id)

    return token


class APIDataLinkersHandler(APIDataConnectorsBase):
    @authenticated
    @with_scope(scopes.DATASOURCES_CREATE)
    @check_rate_limit(Limit.api_connectors_create)
    async def put(self, id=None):
        """
        Update a data linker

        .. sourcecode:: bash
            :caption: Updating a data linker

            curl \\
                -H "Authorization: Bearer $TOKEN" \\
                -X PUT "http://api.tinybird.co/v0/linkers/<linker_or_ds_id>" \\
                --data-urlencode "name=my_connector" \\
                --data-urlencode "service=bigquery" \\
                --data-urlencode "query=select * from test"
                --data-urlencode "mode=replace"

        .. sourcecode:: json
            :caption: Response

            {
                "id": "1234",
                "name" "my_connector",
                "service": "kafka",
                "settings": {
                    "kafka_bootstrap_servers": "kafka_server",
                }
            }
        """
        service_readers = {DataConnectors.SNOWFLAKE: self._get_snowflake_params}

        if not id:
            raise ApiHTTPError.from_request_error(DataConnectorsClientErrorBadRequest.missing_param(param="id"))

        service = self.get_argument("service", None)

        if service is None:
            raise ApiHTTPError.from_request_error(DataConnectorsClientErrorBadRequest.missing_param(param="service"))

        workspace = self.get_workspace_from_db()

        response = {}
        if service == DataConnectors.BIGQUERY:
            bigquery_linker_params = self._get_bigquery_params()
            self.validate_mode(bigquery_linker_params)
            # linker info is included in ds
            datasource = workspace.get_datasource(id)
            if datasource is None or datasource.service is None:
                raise ApiHTTPError.from_request_error(DataConnectorsClientErrorNotFound.no_data_linker())
            elif datasource.service != DataConnectors.BIGQUERY:
                raise ApiHTTPError.from_request_error(DataConnectorsClientErrorBadRequest.data_linker_other_type())

            response = await update_bigquery_linker(workspace, datasource, bigquery_linker_params)
        elif service in service_readers:
            linker = DataLinker.get_by_id(id)
            if not linker:
                datasource = workspace.get_datasource(id)
                try:
                    linker = DataLinker.get_by_datasource_id(datasource.id)
                except Exception:
                    linker = None

            if not linker:
                raise ApiHTTPError.from_request_error(DataConnectorsClientErrorNotFound.no_data_linker())

            if linker.service != service:
                raise ApiHTTPError.from_request_error(DataConnectorsClientErrorBadRequest.data_linker_other_type())

            datasource = workspace.get_datasource(linker.datasource_id)
            data_connector = DataConnector.get_by_id(linker.data_connector_id)
            linker_settings = service_readers[service]()
            self.validate_mode(linker_settings)
            response = await update_generic_linker(
                service, workspace, datasource, data_connector, linker, linker_settings
            )
        else:
            raise ApiHTTPError.from_request_error(DataConnectorsClientErrorNotFound.no_data_connector_type())

        private_settings = {"stage", "tb_token", "ingest_now", "external_data_source"}
        public_response = {key: response[key] for key in response.keys() if key not in private_settings}
        self.write_json(public_response)

    @authenticated
    @check_rate_limit(Limit.api_connectors_list)
    async def get(self, id=None):
        service_readers = {DataConnectors.SNOWFLAKE: self._get_snowflake_params}

        if not id:
            raise ApiHTTPError.from_request_error(DataConnectorsClientErrorBadRequest.missing_param(param="id"))

        workspace = self.get_workspace_from_db()

        service = self.get_argument("service", None)

        if service is None:
            raise ApiHTTPError.from_request_error(DataConnectorsClientErrorBadRequest.missing_param(param="service"))

        if service == DataConnectors.BIGQUERY:
            datasource = workspace.get_datasource(id)
            if datasource is None or datasource.service is None:
                raise ApiHTTPError.from_request_error(DataConnectorsClientErrorNotFound.no_data_linker())
            elif datasource.service != DataConnectors.BIGQUERY:
                raise ApiHTTPError.from_request_error(DataConnectorsClientErrorBadRequest.data_linker_other_type())

            linker_settings = {
                "workspace_name": workspace.name,
                "workspace_id": workspace.id,
                "datasource_name": datasource.name,
                "datasource_id": datasource.id,
                "service": datasource.service,
                "settings": datasource.service_conf,
                "token_name": self._get_token_name(workspace=workspace, datasource=datasource),
            }
            self.write_json(linker_settings)
        elif service in service_readers:
            linker = DataLinker.get_by_id(id)
            if not linker:
                datasource = workspace.get_datasource(id)
                try:
                    linker = DataLinker.get_by_datasource_id(datasource.id)
                except Exception:
                    linker = None

            if not linker:
                raise ApiHTTPError.from_request_error(DataConnectorsClientErrorNotFound.no_data_linker())

            if linker.service != service:
                raise ApiHTTPError.from_request_error(DataConnectorsClientErrorBadRequest.data_linker_other_type())

            datasource = workspace.get_datasource(linker.datasource_id)
            linker_settings = {
                "workspace_name": workspace.name,
                "workspace_id": workspace.id,
                "datasource_name": datasource.name,
                "datasource_id": datasource.id,
                "service": datasource.service,
                "settings": linker.public_settings,
                "token_name": self._get_token_name(workspace=workspace, datasource=datasource),
            }
            self.write_json(linker_settings)
        else:
            raise ApiHTTPError.from_request_error(DataConnectorsClientErrorNotFound.no_data_connector_type())

    def _get_token_name(self, workspace: Workspace, datasource: Datasource) -> Optional[str]:
        access_info = self._get_access_info()
        if not access_info:
            return None
        return access_info.name

    def _get_bigquery_params(self) -> dict:
        keys = ["query", "cron", "mode"]
        return self._get_param_list_from_request(keys)

    def _get_snowflake_params(self) -> dict:
        keys = ["tb_token", "external_data_source", "mode", "query", "cron", "ingest_now"]
        params = self._get_param_list_from_request(keys)
        return params

    def _get_param_list_from_request(self, keys: List[str]) -> dict:
        params = {}
        for key in keys:
            if (value := self.get_argument(key, None)) is not None:
                params[key] = value
        return params

    def validate_mode(self, params: Dict[str, str]):
        mode = params.get("mode", "replace")
        if mode != "replace":
            raise ApiHTTPError.from_request_error(
                DataConnectorsClientErrorBadRequest.invalid_mode(invalid=mode, valid="replace")
            )


def handlers():
    return [
        url(r"/v0/linkers/(.+)", APIDataLinkersHandler),
    ]
